"""
Author: PumpkinğŸƒ
Date:2025-11-07
Description: å›¾å¤„ç†æ¨¡å—
"""

import json
import time
import warnings
from typing import List, Union
import numpy as np
import pandas as pd
import scipy.sparse as sp
from sklearn.metrics import pairwise_distances


def is_consecutive(lst: List[int]) -> bool:
    if not lst:  # å¦‚æœåˆ—è¡¨ä¸ºç©º
        return False

    # æ’åºåˆ—è¡¨
    lst = list(set(lst))
    lst_sorted = sorted(lst)

    # æ£€æŸ¥ç›¸é‚»å…ƒç´ å·®æ˜¯å¦ä¸º 1
    for i in range(1, len(lst_sorted)):
        if lst_sorted[i] - lst_sorted[i - 1] != 1:
            return False

    return True


def is_sparse_based_on_density(matrix: np.ndarray, threshold: float = 0.9) -> bool:
    # åˆ¤æ–­çŸ©é˜µæ˜¯å¦ä¸ºç¨€ç–
    # è®¡ç®—é›¶å…ƒç´ çš„æ¯”ä¾‹
    zero_count = np.sum(matrix == 0)
    total_elements = matrix.size
    zero_density = zero_count / total_elements
    return zero_density > threshold


def edge_process(
    dataname: str, undirected: bool = True, sparsity_threshold: float = 0.9
) -> Union[np.ndarray, sp.csr_matrix]:
    """å¤„ç†å›¾æ•°æ®é›†çš„è¾¹ä¿¡æ¯ï¼Œå¹¶æ„å»ºé‚»æ¥çŸ©é˜µè¡¨ç¤ºã€‚

    è¯¥å‡½æ•°è¯»å–æŒ‡å®šæ•°æ®é›†çš„è¾¹æ–‡ä»¶ï¼Œæ„å»ºå›¾çš„é‚»æ¥çŸ©é˜µï¼Œå¹¶æ ¹æ®ç¨€ç–æ€§åˆ¤æ–­æ˜¯å¦è½¬æ¢ä¸º
    ç¨€ç–çŸ©é˜µè¡¨ç¤ºï¼Œä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡ã€‚

    Args:
        dataname (str): æ•°æ®é›†åç§°ï¼Œç”¨äºç¡®å®šè¾¹æ–‡ä»¶è·¯å¾„
        undirected (bool): å›¾æ˜¯å¦ä¸ºæ— å‘å›¾ï¼Œå¦‚æœä¸ºTrueåˆ™é‚»æ¥çŸ©é˜µå°†æ˜¯å¯¹ç§°çš„ï¼Œé»˜è®¤ä¸ºTrue
        sparsity_threshold (float): åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ç¨€ç–çŸ©é˜µè¡¨ç¤ºçš„é˜ˆå€¼ï¼Œå½“çŸ©é˜µç¨€ç–åº¦è¶…è¿‡æ­¤å€¼æ—¶ä½¿ç”¨ç¨€ç–è¡¨ç¤ºï¼Œé»˜è®¤ä¸º0.9

    Returns:
        Union[np.ndarray, sp.csr_matrix]: æ„å»ºçš„é‚»æ¥çŸ©é˜µï¼Œå¯ä»¥æ˜¯NumPyç¨ å¯†æ•°ç»„æˆ–SciPy CSRæ ¼å¼çš„ç¨€ç–çŸ©é˜µ
    """
    # æ„å»ºè¾¹æ–‡ä»¶è·¯å¾„
    edges_file = f"data/graphs/{dataname}.edges"

    # è¯»å–è¾¹æ•°æ®æ–‡ä»¶
    edges = pd.read_csv(edges_file, header=None)
    edges.columns = ["id1", "id2"]  # ä¸ºè¾¹æ•°æ®æ·»åŠ åˆ—å

    # è½¬æ¢ä¸ºNumPyæ•°ç»„æ ¼å¼ï¼Œä¾¿äºåç»­å¤„ç†
    edges = edges.to_numpy()

    # ç¡®å®šå›¾ä¸­èŠ‚ç‚¹æ•°é‡ï¼ˆå‡è®¾èŠ‚ç‚¹IDè¿ç»­ä¸”ä»0å¼€å§‹ï¼‰
    n = edges.max() + 1

    # åˆå§‹åŒ–é‚»æ¥çŸ©é˜µï¼ˆå…¨é›¶çŸ©é˜µï¼‰
    adj = np.zeros((n, n))

    # å¡«å……é‚»æ¥çŸ©é˜µ
    for u, v in edges:
        adj[u, v] = 1  # è®¾ç½®è¾¹(u,v)å­˜åœ¨
        if undirected:
            adj[v, u] = 1  # æ— å‘å›¾ä¸­ï¼Œè¾¹(v,u)ä¹Ÿå­˜åœ¨

    # æ ¹æ®é›¶å…ƒç´ çš„å¯†åº¦åˆ¤æ–­æ˜¯å¦éœ€è¦è½¬æ¢ä¸ºç¨€ç–çŸ©é˜µï¼Œä»¥èŠ‚çœå†…å­˜
    if is_sparse_based_on_density(adj, sparsity_threshold):
        adj = sp.csr_matrix(adj)  # è½¬æ¢ä¸ºå‹ç¼©ç¨€ç–è¡Œ(CSR)æ ¼å¼
        print("é‚»æ¥çŸ©é˜µè½¬æ¢ä¸ºç¨€ç–çŸ©é˜µ")

    return adj  # è¿”å›æ„å»ºçš„é‚»æ¥çŸ©é˜µ


def feature_process(
    dataname: str, sigma: float = 0.5, sparsity_threshold: float = 0.9
) -> np.ndarray:
    """å¤„ç†å›¾æ•°æ®é›†çš„èŠ‚ç‚¹ç‰¹å¾ï¼Œå¹¶è®¡ç®—åŸºäºé«˜æ–¯æ ¸çš„ç‰¹å¾ç›¸ä¼¼åº¦çŸ©é˜µã€‚

    è¯¥å‡½æ•°è¯»å–æŒ‡å®šæ•°æ®é›†çš„ç‰¹å¾æ–‡ä»¶ï¼Œæ„å»ºèŠ‚ç‚¹-ç‰¹å¾çŸ©é˜µï¼Œæ ¹æ®ç¨€ç–æ€§åˆ¤æ–­æ˜¯å¦è½¬æ¢ä¸ºç¨€ç–è¡¨ç¤ºï¼Œ
    å¹¶æœ€ç»ˆè®¡ç®—åŸºäºé«˜æ–¯æ ¸çš„èŠ‚ç‚¹é—´ç‰¹å¾ç›¸ä¼¼åº¦çŸ©é˜µï¼Œç”¨äºåç»­å›¾è¡¨ç¤ºå­¦ä¹ ã€‚

    Args:
        dataname (str): æ•°æ®é›†åç§°ï¼Œç”¨äºç¡®å®šç‰¹å¾æ–‡ä»¶è·¯å¾„
        sigma (float): é«˜æ–¯æ ¸å‡½æ•°çš„å¸¦å®½å‚æ•°ï¼Œæ§åˆ¶ç›¸ä¼¼åº¦è¡°å‡é€Ÿåº¦ï¼Œé»˜è®¤ä¸º0.5
        sparsity_threshold (float): åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ç¨€ç–çŸ©é˜µè¡¨ç¤ºçš„é˜ˆå€¼ï¼Œå½“çŸ©é˜µç¨€ç–åº¦è¶…è¿‡æ­¤å€¼æ—¶ä½¿ç”¨ç¨€ç–è¡¨ç¤ºï¼Œé»˜è®¤ä¸º0.9

    Returns:
        np.ndarray: èŠ‚ç‚¹é—´çš„ç‰¹å¾ç›¸ä¼¼åº¦çŸ©é˜µï¼Œå½¢çŠ¶ä¸º[èŠ‚ç‚¹æ•°é‡, èŠ‚ç‚¹æ•°é‡]
    """
    # æ„å»ºç‰¹å¾æ–‡ä»¶è·¯å¾„
    features_file = f"data/graphs/{dataname}.features"

    # è¯»å–ç‰¹å¾æ–‡ä»¶æ•°æ®
    with open(features_file, "r") as f:
        features_data = json.load(f)  # åŠ è½½JSONæ ¼å¼çš„ç‰¹å¾æ•°æ®
        nodes = list(features_data.keys())  # è·å–æ‰€æœ‰èŠ‚ç‚¹çš„åˆ—è¡¨

    num_nodes = len(nodes)  # è®¡ç®—èŠ‚ç‚¹æ•°é‡

    # è·å–æ‰€æœ‰ç‰¹å¾çš„æœ€å¤§ç´¢å¼•å€¼ï¼Œç¡®å®šç‰¹å¾æ€»æ•°
    all_features = sorted([f for features in features_data.values() for f in features])
    if not is_consecutive(all_features):
        warnings.warn("ç‰¹å¾ç´¢å¼•ä¸æ˜¯è¿ç»­çš„æ•´æ•°ï¼Œä½†æ˜¯ä¾æ—§å¯ä»¥è®¡ç®—ã€‚")

    num_features = (
        max(all_features) + 1
    )  # å› ä¸ºç‰¹å¾ä» 0 å¼€å§‹ç´¢å¼•ï¼Œæ‰€ä»¥ç‰¹å¾æ€»æ•°ä¸ºæœ€å¤§ç´¢å¼•+1

    # åˆ›å»ºç¨€ç–ç‰¹å¾çŸ©é˜µï¼ˆåˆå§‹ä¸ºç¨ å¯†çŸ©é˜µï¼‰
    features = np.zeros((num_nodes, num_features))
    for i, node in enumerate(nodes):
        for feature in features_data[node]:
            features[i, feature] = 1  # ç‰¹å¾å­˜åœ¨åˆ™æ ‡è®°ä¸º1ï¼Œä¸å­˜åœ¨ä¸º0

    # æ ¹æ®é›¶å…ƒç´ çš„å¯†åº¦åˆ¤æ–­æ˜¯å¦éœ€è¦è½¬æ¢ä¸ºç¨€ç–çŸ©é˜µï¼Œä»¥èŠ‚çœå†…å­˜å’ŒåŠ é€Ÿè®¡ç®—
    if is_sparse_based_on_density(features, sparsity_threshold):
        features = sp.csr_matrix(features)  # è½¬æ¢ä¸ºå‹ç¼©ç¨€ç–è¡Œ(CSR)æ ¼å¼
        print("ç‰¹å¾çŸ©é˜µè½¬æ¢ä¸ºç¨€ç–çŸ©é˜µ")

    # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆä½¿ç”¨é«˜æ–¯æ ¸å‡½æ•°ï¼‰
    if sp.issparse(features):
        # ç¨€ç–çŸ©é˜µè®¡ç®—ç­–ç•¥
        features_sq = features.power(2).sum(axis=1).A1  # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹ç‰¹å¾çš„å¹³æ–¹å’Œ
        dot_product = features @ features.T  # è®¡ç®—èŠ‚ç‚¹é—´ç‰¹å¾çš„ç‚¹ç§¯
        # ä½¿ç”¨ç‚¹ç§¯è®¡ç®—æ¬§æ°è·ç¦»çš„å¹³æ–¹
        dists_sq = (
            features_sq[:, None] + features_sq[None, :] - 2 * dot_product.toarray()
        )
    else:
        # ç¨ å¯†çŸ©é˜µç›´æ¥è®¡ç®—æ¬§æ°è·ç¦»çš„å¹³æ–¹
        dists_sq = pairwise_distances(features, metric="sqeuclidean")

    # åº”ç”¨é«˜æ–¯æ ¸å‡½æ•°å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦
    similarity_matrix = np.exp(-sigma * dists_sq)

    # æ£€æŸ¥ç›¸ä¼¼åº¦çŸ©é˜µä¸­çš„ NaN å’Œ inf å€¼ï¼Œé¿å…åç»­è®¡ç®—é”™è¯¯
    if np.any(np.isnan(similarity_matrix)) or np.any(np.isinf(similarity_matrix)):
        warnings.warn("ç›¸ä¼¼åº¦çŸ©é˜µä¸­åŒ…å« NaN æˆ– inf å€¼ï¼")

    return similarity_matrix  # è¿”å›è®¡ç®—å¾—åˆ°çš„ç‰¹å¾ç›¸ä¼¼åº¦çŸ©é˜µ


def high_order(
    term: Union[np.ndarray, sp.csr_matrix], order: int = 2, decay: float = 0.5
) -> np.ndarray:
    if sp.issparse(term):  # å¦‚æœæ˜¯ç¨€ç–çŸ©é˜µ
        ho_matrix = sp.csr_matrix(term.shape, dtype=np.float32)  # åˆå§‹åŒ–é«˜é˜¶çŸ©é˜µ
        matrix_power = term.copy()  # å½“å‰çŸ©é˜µçš„å¹‚ï¼Œåˆå§‹ä¸º term
        factorial = 1.0
        for i in range(1, order + 1):
            factorial *= i
            ho_matrix += matrix_power.multiply(decay**i / factorial)  # ç´¯åŠ é«˜é˜¶é¡¹
            matrix_power = matrix_power @ term  # æ›´æ–°çŸ©é˜µçš„å¹‚
    else:  # å¦‚æœæ˜¯ç¨ å¯†çŸ©é˜µ
        ho_matrix = np.zeros_like(term, dtype=np.float32)  # åˆå§‹åŒ–é«˜é˜¶çŸ©é˜µ
        matrix_power = term.copy()  # å½“å‰çŸ©é˜µçš„å¹‚ï¼Œåˆå§‹ä¸º term
        factorial = 1.0
        for i in range(1, order + 1):
            factorial *= i
            ho_matrix += matrix_power * (decay**i / factorial)  # ç´¯åŠ é«˜é˜¶é¡¹
            matrix_power = matrix_power @ term  # æ›´æ–°çŸ©é˜µçš„å¹‚
    if sp.issparse(ho_matrix):
        ho_matrix = ho_matrix.toarray()
    return ho_matrix


if __name__ == "__main__":
    t = time.time()
    edge_process("citeseer")
    print(f"é‚»æ¥çŸ©é˜µå¤„ç†è€—æ—¶: {time.time() - t}")
    t = time.time()
    feature_process("citeseer")
    print(f"ç‰¹å¾çŸ©é˜µå¤„ç†è€—æ—¶: {time.time() - t}")
    t = time.time()
    high_order(edge_process("citeseer"))
    print(f"é«˜é˜¶ä¼ æ’­çŸ©é˜µå¤„ç†è€—æ—¶: {time.time() - t}")
