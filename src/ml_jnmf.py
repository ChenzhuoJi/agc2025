import numpy as np
import pandas as pd
from sklearn.decomposition._nmf import _initialize_nmf
from sklearn.cluster import KMeans
from rich.console import Console

from src.helpers import compute_communitude_metric


class EarlyStopping:
    def __init__(self, patience=10, min_delta=1e-4, max_loss=1e10):
        self.patience = patience
        self.min_delta = min_delta
        self.max_loss = max_loss  # æ·»åŠ æœ€å¤§æŸå¤±å€¼é™åˆ¶
        self.counter = 0

    def step(self, current_loss, best_loss):
        # æ£€æŸ¥å½“å‰æŸå¤±å€¼æ˜¯å¦è¶…è¿‡æœ€å¤§å®¹å¿å€¼
        if current_loss >= self.max_loss:
            return True

        # åŸæœ‰çš„æ—©åœé€»è¾‘
        if current_loss < best_loss - self.min_delta:
            self.counter = 0
        else:
            self.counter += 1
        return self.counter >= self.patience

    def reset(self):
        self.counter = 0


class ConvergenceChecker:
    def __init__(self, patience=10, tol=1e-4):
        self.patience = patience  # è§‚å¯Ÿæœ€è¿‘å¤šå°‘æ¬¡ loss
        self.tol = tol  # åˆ¤å®šæ”¶æ•›çš„é˜ˆå€¼
        self.history = []  # ä¿å­˜æœ€è¿‘çš„ loss
        self.is_converged = False  # æ˜¯å¦å·²ç»æ”¶æ•›

    def step(self, current_loss):
        self.history.append(current_loss)
        if len(self.history) > self.patience:
            self.history.pop(0)

        # åˆ¤å®šæ”¶æ•›ï¼šæœ€è¿‘ patience æ¬¡ loss å˜åŒ–éƒ½å°äº tol
        if len(self.history) == self.patience:
            diffs = [
                abs(self.history[i] - self.history[i - 1])
                / (self.history[i - 1] + 1e-10)
                for i in range(1, len(self.history))
            ]
            self.is_converged = all(diff < self.tol for diff in diffs)

        return self.is_converged

    def reset(self):
        self.history = []
        self.is_converged = False


class ML_JNMF:
    """
    Core implementation of Multi-Level Joint Non-negative Matrix Factorization (ML-JNMF).
    The algorithm jointly factorizes intra-layer and inter-layer adjacency matrices
    to learn shared latent embeddings across multiple networks.
    """

    def __init__(
        self,
        mu1=1.0,
        mu2=2.0,
        max_iter=300,
        tol=1e-4,
        patience=20,
        min_delta=1e-4,
        random_state=42,
    ):
        """
        Args:
            mu1 (float): Weight for cross-layer reconstruction constraint.
            mu2 (float): Weight for intra/inter-layer embedding similarity constraint.
            max_iter (int): Maximum number of update iterations.
            tol (float): Relative tolerance for convergence check.
            patience (int): Number of iterations to wait for improvement before early stopping.
            min_delta (float): Minimum relative improvement in loss required to stop.
        """
        self.mu1 = mu1
        self.mu2 = mu2
        self.max_iter = max_iter
        self.tol = tol
        self.random_state = random_state
        self.early_stopper = EarlyStopping(patience=patience, min_delta=min_delta)
        self.convergence_checker = ConvergenceChecker(
            patience=patience,
        )
        # æ¨¡å‹çš„ç»“æœ
        self.U1, self.U2, self.B1, self.B2, self.S12 = [None] * 5
        self.loss_history = []
        self.final_loss = None
        self.is_early_stopped = False


    def _initialize(self, A1, A2, A12, r):
        """Initialize U1, U2, B1, B2, S12 using NNDSVDAR for stability."""
        U1, _ = _initialize_nmf(
            A1, n_components=r, init="nndsvdar", random_state=self.random_state
        )
        U2, _ = _initialize_nmf(
            A2, n_components=r, init="nndsvdar", random_state=self.random_state
        )
        B1, B2t = _initialize_nmf(
            A12, n_components=r, init="nndsvdar", random_state=self.random_state
        )
        B2 = B2t.T
        # å¤„ç† NaN å€¼
        U1 = np.nan_to_num(U1, nan=1e-6)
        U2 = np.nan_to_num(U2, nan=1e-6)
        B1 = np.nan_to_num(B1, nan=1e-6)
        B2 = np.nan_to_num(B2, nan=1e-6)

        # ceil = 2
        # U1 = np.random.uniform(0, ceil, size=(A1.shape[0], r))
        # U2 = np.random.uniform(0, ceil, size=(A2.shape[0], r))
        # B1 = np.random.uniform(0, ceil, size=(A1.shape[1], r))
        # B2 = np.random.uniform(0, ceil, size=(A2.shape[1], r))
        S12 = np.eye(r)
        return U1, U2, B1, B2, S12

    def _compute_loss(self, A1, A2, A12, U1, U2, B1, B2, S12):
        """Compute the total objective function of ML-JNMF."""

        # è®¡ç®— L2,1 èŒƒæ•°çš„è¾…åŠ©å‡½æ•°
        def l21_norm(X):
            return np.sum(np.sqrt(np.sum(X**2, axis=1)))

        # -------- Intra loss (use L2,1 norm, no square) --------
        intra_loss = l21_norm(A1 - U1 @ U1.T) + l21_norm(A2 - U2 @ U2.T)

        # -------- Inter loss (use L2,1 norm, no square) --------
        inter_loss = self.mu1 * l21_norm(A12 - B1 @ S12 @ B2.T)

        # -------- Sim loss (still Frobenius norm squared) --------
        sim_loss = self.mu2 * (
            np.linalg.norm(U1 @ U1.T - B1 @ B1.T, "fro") ** 2
            + np.linalg.norm(U2 @ U2.T - B2 @ B2.T, "fro") ** 2
        )

        total_loss = intra_loss + inter_loss + sim_loss
        if np.isnan(total_loss) or np.isinf(total_loss):
            print("âš ï¸ Loss NaN detected")
            print("A1:", np.isnan(A1).any(), "A2:", np.isnan(A2).any(), "A12:", np.isnan(A12).any())
            print("U1:", np.isnan(U1).any(), "U2:", np.isnan(U2).any())
            print("B1:", np.isnan(B1).any(), "B2:", np.isnan(B2).any(), "S12:", np.isnan(S12).any())
            print("Any Inf:", np.isinf(U1).any() or np.isinf(U2).any() or np.isinf(B1).any())
            raise ValueError("NaN detected in loss computation")
        return total_loss

    def _update(self, A1, A2, A12, U1, U2, B1, B2, S12, eps=1e-10):
        """æ‰§è¡Œä¸€æ¬¡æ‰€æœ‰å› å­çš„ä¹˜æ³•æ›´æ–°è¿­ä»£

        æ­¤æ–¹æ³•æ˜¯ML-JNMFç®—æ³•çš„æ ¸å¿ƒæ›´æ–°æ­¥éª¤ï¼Œå®ç°äº†éè´ŸçŸ©é˜µåˆ†è§£ä¸­çš„ä¹˜æ³•æ›´æ–°è§„åˆ™ï¼Œ
        ç”¨äºè¿­ä»£ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚

        å‚æ•°:
            A1: ç¬¬ä¸€å±‚ç½‘ç»œçš„é‚»æ¥çŸ©é˜µ
            A2: ç¬¬äºŒå±‚ç½‘ç»œçš„é‚»æ¥çŸ©é˜µ
            A12: å±‚é—´è¿æ¥çš„é‚»æ¥çŸ©é˜µ
            U1: ç¬¬ä¸€å±‚ç½‘ç»œçš„æ½œåœ¨ç‰¹å¾çŸ©é˜µï¼ˆå¾…æ›´æ–°ï¼‰
            U2: ç¬¬äºŒå±‚ç½‘ç»œçš„æ½œåœ¨ç‰¹å¾çŸ©é˜µï¼ˆå¾…æ›´æ–°ï¼‰
            B1: ç¬¬ä¸€å±‚ç½‘ç»œçš„å…±äº«åµŒå…¥çŸ©é˜µï¼ˆå¾…æ›´æ–°ï¼‰
            B2: ç¬¬äºŒå±‚ç½‘ç»œçš„å…±äº«åµŒå…¥çŸ©é˜µï¼ˆå¾…æ›´æ–°ï¼‰
            S12: å±‚é—´æ˜ å°„çŸ©é˜µï¼ˆå¾…æ›´æ–°ï¼‰
            eps: é˜²æ­¢é™¤é›¶é”™è¯¯çš„å°å€¼ï¼Œé»˜è®¤ä¸º1e-10

        è¿”å›å€¼:
            tuple: åŒ…å«æ›´æ–°åçš„çŸ©é˜µ (U1, U2, B1, B2, S12)
        """

        def build_Z(A, U):
            """æ„å»ºå¯¹è§’çŸ©é˜µ Z = diag(1 / ||A - UUáµ€||â‚‚)ï¼Œç”¨äºåŠ æƒé‡æ„è¯¯å·®

            å‚æ•°:
                A: åŸå§‹é‚»æ¥çŸ©é˜µ
                U: æ½œåœ¨ç‰¹å¾çŸ©é˜µ

            è¿”å›å€¼:
                numpy.ndarray: å¯¹è§’æƒé‡çŸ©é˜µ
            """
            # è®¡ç®—æ®‹å·®çŸ©é˜µï¼ˆåŸå§‹çŸ©é˜µä¸é‡æ„çŸ©é˜µçš„å·®ï¼‰
            residual = A - U @ U.T
            # è®¡ç®—æ¯è¡Œçš„L2èŒƒæ•°ï¼Œå¹¶æ·»åŠ å°å€¼é˜²æ­¢é™¤é›¶
            norms = np.linalg.norm(residual, axis=1) + eps
            # æ„å»ºå¯¹è§’çŸ©é˜µï¼Œå¯¹è§’çº¿å…ƒç´ ä¸ºèŒƒæ•°çš„å€’æ•°
            return np.diag(1 / norms)

        def build_Z12(A12, B1, B2, S12):
            """ä¸ºå±‚é—´é‡æ„æ„å»ºå¯¹è§’çŸ©é˜µ

            å‚æ•°:
                A12: å±‚é—´è¿æ¥çš„é‚»æ¥çŸ©é˜µ
                B1: ç¬¬ä¸€å±‚ç½‘ç»œçš„å…±äº«åµŒå…¥çŸ©é˜µ
                B2: ç¬¬äºŒå±‚ç½‘ç»œçš„å…±äº«åµŒå…¥çŸ©é˜µ
                S12: å±‚é—´æ˜ å°„çŸ©é˜µ

            è¿”å›å€¼:
                numpy.ndarray: å¯¹è§’æƒé‡çŸ©é˜µ
            """
            # è®¡ç®—å±‚é—´æ®‹å·®çŸ©é˜µ
            residual = A12 - B1 @ S12 @ B2.T
            # è®¡ç®—æ¯è¡Œçš„L2èŒƒæ•°ï¼Œå¹¶æ·»åŠ å°å€¼é˜²æ­¢é™¤é›¶
            norms = np.linalg.norm(residual, axis=1) + eps
            # æ„å»ºå¯¹è§’çŸ©é˜µï¼Œå¯¹è§’çº¿å…ƒç´ ä¸ºèŒƒæ•°çš„å€’æ•°
            return np.diag(1 / norms)

        # æ„å»ºä¸‰å±‚çš„æƒé‡çŸ©é˜µZ1ã€Z2å’ŒZ12
        Z1, Z2, Z12 = build_Z(A1, U1), build_Z(A2, U2), build_Z12(A12, B1, B2, S12)

        # æ›´æ–°U1çŸ©é˜µï¼šä½¿ç”¨ä¹˜æ³•æ›´æ–°è§„åˆ™ï¼Œç¡®ä¿éè´Ÿæ€§
        # åˆ†å­éƒ¨åˆ†ï¼šåŒ…å«é‡æ„è¯¯å·®å’Œä¸B1çš„ä¸€è‡´æ€§çº¦æŸ
        U1_num = (Z1 @ A1 @ U1 + A1 @ Z1 @ U1 + 2 * self.mu2 * B1 @ B1.T @ U1) * U1
        # åˆ†æ¯éƒ¨åˆ†ï¼šåŒ…å«å½’ä¸€åŒ–é¡¹
        U1_den = (
            U1 @ U1.T @ Z1 @ U1
            + Z1 @ U1 @ U1.T @ U1
            + 2 * self.mu2 * U1 @ U1.T @ U1
            + eps
        )
        # æ‰§è¡Œæ›´æ–°
        U1 = U1_num / U1_den

        # æ›´æ–°U2çŸ©é˜µï¼šä¸U1ç±»ä¼¼çš„æ›´æ–°è§„åˆ™
        U2_num = (Z2 @ A2 @ U2 + A2 @ Z2 @ U2 + 2 * self.mu2 * B2 @ B2.T @ U2) * U2
        U2_den = (
            U2 @ U2.T @ Z2 @ U2
            + Z2 @ U2 @ U2.T @ U2
            + 2 * self.mu2 * U2 @ U2.T @ U2
            + eps
        )
        U2 = U2_num / U2_den

        # æ›´æ–°B1çŸ©é˜µï¼šç»“åˆå±‚é—´é‡æ„å’Œä¸U1çš„ä¸€è‡´æ€§çº¦æŸ
        B1_num = (
            self.mu1 * A12 @ Z12 @ B2 @ S12.T + 2 * self.mu2 * U1 @ U1.T @ B1
        ) * B1
        B1_den = (
            self.mu1 * B1 @ S12 @ B2.T @ Z12 @ B2 @ S12.T
            + 2 * self.mu2 * B1 @ B1.T @ B1
            + eps
        )
        B1 = B1_num / B1_den

        # æ›´æ–°B2çŸ©é˜µï¼šä¸B1ç±»ä¼¼çš„æ›´æ–°è§„åˆ™
        B2_num = (
            self.mu1 * Z12 @ A12.T @ B1 @ S12 + 2 * self.mu2 * U2 @ U2.T @ B2
        ) * B2
        B2_den = (
            self.mu1 * Z12 @ B2 @ S12.T @ B1.T @ B1 @ S12
            + 2 * self.mu2 * B2 @ B2.T @ B2
            + eps
        )
        B2 = B2_num / B2_den

        # æ›´æ–°S12çŸ©é˜µï¼šå±‚é—´æ˜ å°„çŸ©é˜µçš„æ›´æ–°è§„åˆ™
        S12_num = B1.T @ A12 @ Z12 @ B2
        S12_den = B1.T @ B1 @ S12 @ B2.T @ Z12 @ B2 + eps
        S12 = (S12_num / S12_den) * S12

        # è¿”å›æ‰€æœ‰æ›´æ–°åçš„çŸ©é˜µ
        return U1, U2, B1, B2, S12

    def fit(self, A1, A2, A12, r):
        """
        Fit ML-JNMF on given adjacency matrices using EarlyStopping and ConvergenceChecker.
        """
        console = Console()
        # Initialize matrices
        self.A1, self.A2, self.A12 = A1, A2, A12
        self.U1, self.U2, self.B1, self.B2, self.S12 = self._initialize(A1, A2, A12, r)

        # åˆå§‹åŒ–è®­ç»ƒç®¡ç†å™¨
        best_loss = float("inf")
        best_params = None
        self.early_stopper.reset()
        self.convergence_checker.reset()

        for it in range(self.max_iter):
            # è®¡ç®—å½“å‰ loss
            loss = self._compute_loss(
                self.A1, self.A2, self.A12, self.U1, self.U2, self.B1, self.B2, self.S12
            )
            self.loss_history.append(loss)
            # console.print(
            #     f"iteration={it+1}, loss={loss:.4f}, best_loss={best_loss:.4f}",
            #     style="bold blue",
            # )
            # ğŸ” Early stopping æ£€æŸ¥æ”¾åœ¨æ›´æ–° best_loss ä¹‹å‰
            if self.early_stopper.step(loss, best_loss):
                # console.print(f"loss={loss:.4f}, best_loss={best_loss:.4f}")
                self.U1, self.U2, self.B1, self.B2, self.S12 = best_params
                self.is_early_stopped = True
                self.final_loss = best_loss
                console.print(
                    f"[Early Stop] iteration={it+1}, best_loss={best_loss:.4f} at iteration {best_it+1}, n_nodes={A1.shape[0]}",
                    style="bold yellow",
                )
                break

            # âœ… åœ¨ step() ä¹‹åå†æ›´æ–° best_loss
            if loss < best_loss:
                best_loss = loss
                best_params = (
                    self.U1.copy(),
                    self.U2.copy(),
                    self.B1.copy(),
                    self.B2.copy(),
                    self.S12.copy(),
                )
                best_it = it

            # æ”¶æ•›æ€§æ£€æŸ¥
            if self.convergence_checker.step(loss):
                self.U1, self.U2, self.B1, self.B2, self.S12 = best_params
                self.final_loss = best_loss
                console.print(
                    f"[Converged] iteration={it+1}, loss={loss:.4f}",
                    style="bold green",
                )
                break

            # æ›´æ–°å› å­çŸ©é˜µ
            self.U1, self.U2, self.B1, self.B2, self.S12 = self._update(
                A1, A2, A12, self.U1, self.U2, self.B1, self.B2, self.S12
            )

        # å¦‚æœå¾ªç¯è‡ªç„¶ç»“æŸï¼Œä¹Ÿä½¿ç”¨æœ€ä½³å‚æ•°
        else:
            self.U1, self.U2, self.B1, self.B2, self.S12 = best_params
            self.final_loss = best_loss
            console.print(
                f"[End] iteration={it+1}, loss={loss:.4f}",
                style="bold red",
            )
        return self

    def predict(self, r, pred_method, lamb=None):
        """
        åŸºäºè®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°é¢„æµ‹ç¤¾åŒºç»“æ„
        
        å‚æ•°:
        r: int - ç¤¾åŒºæ•°é‡/èšç±»æ•°ç›®
        pred_method: str - é¢„æµ‹æ–¹æ³•ï¼Œå¯é€‰å€¼åŒ…æ‹¬ 'kmeans' å’Œ 'laplace'
        lamb: float (å¯é€‰) - ç”¨äºåŠ æƒç»„åˆU1å’ŒU2çš„å‚æ•°ï¼ŒèŒƒå›´åœ¨[0,1]ä¹‹é—´
        """
        if pred_method == "kmeans":
            kmeans = KMeans(n_clusters=r, random_state=self.random_state)
            Z = lamb * self.U1 + (1 - lamb) * self.U2
            S = np.dot(Z, Z.T)
            return kmeans.fit_predict(Z)

        elif pred_method == "laplace":
            Z = lamb * self.U1 + (1 - lamb) * self.U2
            S = np.dot(Z, Z.T)
            D = np.sum(S, axis=1)
            diag_inv_sqrt = np.where(D > 1e-10, 1.0 / np.sqrt(D), 0.0)
            D_inv_sqrt = np.diag(diag_inv_sqrt)
            L = np.eye(S.shape[0]) - D_inv_sqrt.dot(S).dot(D_inv_sqrt)
            _, eigenvectors = np.linalg.eigh(L)

            Y = eigenvectors[:, :r]  # æ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªç‰¹å¾å‘é‡
            kmeans = KMeans(n_clusters=r, random_state=self.random_state)
            return kmeans.fit_predict(Y)

        # -------- Obtain intra-layer and inter-layer community labels --------
        label_intra_1 = np.argmax(self.U1, axis=1)
        label_inter_1 = np.argmax(self.B1, axis=1)
        label_intra_2 = np.argmax(self.U2, axis=1)
        label_inter_2 = np.argmax(self.B2, axis=1)

        # -------- Calculate community metrics --------
        comm_intra_1 = compute_communitude_metric(self.A1, label_intra_1)
        comm_intra_2 = compute_communitude_metric(self.A2, label_intra_2)
        comm_inter_1 = compute_communitude_metric(self.A12, label_inter_1, axis=0)
        comm_inter_2 = compute_communitude_metric(self.A12, label_inter_2, axis=1)

        # -------- Determine final community type based on metrics --------
        final_community_1 = []
        final_community_2 = []

        for i in range(len(label_intra_1)):
            if comm_inter_1[label_inter_1[i]] > comm_intra_1[label_intra_1[i]]:
                final_community_1.append(("inter", label_inter_1[i]))
            else:
                final_community_1.append(("intra", label_intra_1[i]))

        for j in range(len(label_intra_2)):
            if comm_inter_2[label_inter_2[j]] > comm_intra_2[label_intra_2[j]]:
                final_community_2.append(("inter", label_inter_2[j]))
            else:
                final_community_2.append(("intra", label_intra_2[j]))

        df1 = pd.DataFrame(final_community_1, columns=["type", "community_id"])
        df1.insert(0, "node_id", range(len(final_community_1)))
        df2 = pd.DataFrame(final_community_2, columns=["type", "community_id"])
        df2.insert(0, "node_id", range(len(final_community_2)))
        return df1, df2

    def fit_predict(
        self,
        A1,
        A2,
        A12,
        r,
        pred_method,
        lamb=None,
    ):
        self.fit(A1, A2, A12, r)
        cluster_labels = self.predict(r, pred_method, lamb)
        return cluster_labels
