"""
Author: PumpkinğŸƒ
Date:2025-10-31
Description:evaluator, è¯„ä¼°å™¨
"""

import json
from itertools import combinations
from collections import defaultdict

import numpy as np
from scipy.special import comb
from sklearn.metrics import (
    adjusted_rand_score,
    normalized_mutual_info_score,
    fowlkes_mallows_score,
    homogeneity_completeness_v_measure,
)

from scipy.optimize import linear_sum_assignment

from src.helpers import json2featmat


class internalEvaluator:
    def __init__(self, cluster_labels, edges: np.ndarray, features: np.ndarray):
        """
        èšç±»è¯„ä¼°ç±»ï¼šç”¨äºè¯„ä¼°èšç±»ç»“æœçš„å†…éƒ¨æŒ‡æ ‡

        å‚æ•°:
        cluster_labels (list or array): æ¯ä¸ªèŠ‚ç‚¹çš„é¢„æµ‹èšç±»æ ‡ç­¾
        edges (np.ndarray): å›¾çš„è¾¹æ•°æ®ï¼Œå½¢çŠ¶ä¸º (num_edges, 2)
        features (np.ndarray): èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (num_nodes, feature_dim)
        """
        self.cluster_labels = np.array(cluster_labels)
        self.cluster_labels = np.squeeze(self.cluster_labels)
        if self.cluster_labels.ndim != 1:
            raise ValueError(
                f"cluster_labels å¿…é¡»æ˜¯ä¸€ç»´æ•°ç»„ï¼Œå½“å‰å½¢çŠ¶: {self.cluster_labels.shape}"
            )

        self.edges = np.array(edges)
        self.features = np.array(features)
        self.feature_dim = self.features.shape[1]
        self.num_nodes = len(self.cluster_labels)

        # 2. é¢„è®¡ç®—æ¯ä¸ªç°‡åŒ…å«çš„èŠ‚ç‚¹é›†åˆ
        self.pred_clusters = defaultdict(set)
        for node_id, label in enumerate(self.cluster_labels):
            if isinstance(label, (np.ndarray, list)):
                raise ValueError(
                    f"cluster_labels å…ƒç´ ä¸èƒ½æ˜¯æ•°ç»„/åˆ—è¡¨ï¼Œå½“å‰å…ƒç´ : {label}"
                )
            self.pred_clusters[label].add(node_id)
        self.cluster_ids = list(self.pred_clusters.keys())

        # 3. é¢„è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„é‚»å±…é›†åˆï¼ŒåŠ é€Ÿè¾¹çš„æŸ¥æ‰¾
        self.neighbors = defaultdict(set)
        for u, v in self.edges:
            # ç¡®ä¿ u å’Œ v æ˜¯æ•´æ•°ç±»å‹
            self.neighbors[int(u)].add(int(v))
            self.neighbors[int(v)].add(int(u))

    def normalized_homogeneity(self):
        """
        è®¡ç®—å½’ä¸€åŒ–åŒè´¨æ€§ (Normalized Homogeneity, NorHo)ã€‚

        å…¬å¼: NorHo = (1 / (N * p)) * sum_{k} [n_k * H(C_k)]
        å…¶ä¸­:
        - N æ˜¯æ€»èŠ‚ç‚¹æ•°ã€‚
        - p æ˜¯ç‰¹å¾ç»´åº¦ã€‚
        - n_k æ˜¯ç¬¬ k ä¸ªç°‡çš„èŠ‚ç‚¹æ•°ã€‚
        - H(C_k) æ˜¯ç¬¬ k ä¸ªç°‡çš„åŒè´¨æ€§ï¼ŒH(C_k) = -sum_{j=1 to p} c_{kj} * (1 - c_{kj})
        - c_{kj} æ˜¯ç¬¬ k ä¸ªç°‡ä¸­ï¼Œç‰¹å¾ j ä¸ºéé›¶å€¼çš„èŠ‚ç‚¹æ¯”ä¾‹ã€‚

        è¿”å›:
        float: å½’ä¸€åŒ–åŒè´¨æ€§çš„å€¼ã€‚å¦‚æœæ²¡æœ‰æä¾›ç‰¹å¾æ•°æ®ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸ã€‚
        """
        if self.features is None or self.feature_dim == 0:
            raise RuntimeError("è®¡ç®—åŒè´¨æ€§éœ€è¦ç‰¹å¾æ•°æ®ï¼Œè¯·åœ¨åˆå§‹åŒ–æ—¶æä¾› featuresã€‚")

        total_sum = 0.0
        N = self.num_nodes
        p = self.feature_dim

        for label in self.cluster_ids:
            cluster_nodes = list(self.pred_clusters[label])  # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿ç´¢å¼•
            n_k = len(cluster_nodes)

            if n_k == 0:
                continue

            # æå–ç°‡å†…èŠ‚ç‚¹çš„ç‰¹å¾
            cluster_features = self.features[cluster_nodes]

            # --- è®¡ç®—å•ä¸ªç°‡ C_k çš„åŒè´¨æ€§ H(C_k) ---
            h_c_k = 0.0
            for j in range(p):  # éå†æ¯ä¸ªç‰¹å¾ç»´åº¦
                # è®¡ç®—ç°‡å†…å…·æœ‰è¯¥ç‰¹å¾ï¼ˆå€¼éé›¶ï¼‰çš„èŠ‚ç‚¹æ•°
                count_j = np.count_nonzero(cluster_features[:, j])

                # è®¡ç®—æ¯”ä¾‹ c_kj
                c_kj = count_j / n_k if n_k > 0 else 0.0
                # ç´¯åŠ é¡¹
                h_c_k += c_kj * (1 - c_kj)

            # H(C_k) æ˜¯ç´¯åŠ é¡¹çš„è´Ÿå€¼
            h_c_k = -h_c_k

            # ç´¯åŠ åˆ°æ€»å’Œ
            total_sum += n_k * h_c_k

        # è®¡ç®—æœ€ç»ˆçš„å½’ä¸€åŒ–åŒè´¨æ€§
        denominator = N * p
        if denominator == 0:
            return 0.0

        nor_ho = total_sum / denominator
        return nor_ho

    def _calculate_L_in(self, cluster_nodes):
        """
        è®¡ç®—ä¸€ä¸ªç°‡çš„å†…éƒ¨è¾¹æ•° L_in
        """
        l_in = 0
        # å°†é›†åˆè½¬æ¢ä¸ºåˆ—è¡¨ï¼Œæ–¹ä¾¿éå†
        nodes_list = list(cluster_nodes)
        # éå†ç°‡å†…æ‰€æœ‰èŠ‚ç‚¹å¯¹ (i, j) ä¸” i < jï¼Œé¿å…é‡å¤è®¡ç®—
        for i in range(len(nodes_list)):
            for j in range(i + 1, len(nodes_list)):
                u, v = nodes_list[i], nodes_list[j]
                # æ£€æŸ¥ u æ˜¯å¦åœ¨ v çš„é‚»å±…åˆ—è¡¨ä¸­
                if u in self.neighbors[v]:
                    l_in += 1
        return l_in

    def _calculate_L_out(self, cluster_nodes):
        """
        è®¡ç®—ä¸€ä¸ªç°‡çš„å¤–éƒ¨è¾¹æ•° L_out
        """
        l_out = 0
        # ç°‡å¤–èŠ‚ç‚¹é›†åˆ
        all_nodes_set = set(range(self.num_nodes))
        external_nodes = all_nodes_set - cluster_nodes

        for u in cluster_nodes:
            # u çš„é‚»å±…ä¸­å±äºç°‡å¤–çš„èŠ‚ç‚¹æ•°é‡
            # ä½¿ç”¨é›†åˆçš„äº¤é›†æ“ä½œé«˜æ•ˆè®¡ç®—
            l_out += len(self.neighbors[u].intersection(external_nodes))

        # æ¯æ¡å¤–éƒ¨è¾¹è¢«è®¡ç®—äº†ä¸¤æ¬¡ (u->v å’Œ v->u)ï¼Œå› æ­¤éœ€è¦é™¤ä»¥ 2
        return l_out // 2

    def normalized_tightness(self):
        """
        è®¡ç®—å½’ä¸€åŒ–ç´§å¯†åº¦ (Normalized Tightness, NorTi)

        å…¬å¼: NorTi = (1 / sum(n_k)) * sum( n_k * ( 2*L_in_k/(n_k^2) - L_out_k/(n_k*(N-n_k)) ) )

        è¿”å›:
        float: å½’ä¸€åŒ–ç´§å¯†åº¦çš„å€¼
        """
        total_n_k = 0
        sum_terms = 0.0

        for label in self.cluster_ids:
            cluster_nodes = self.pred_clusters[label]
            n_k = len(cluster_nodes)

            # å¦‚æœç°‡çš„å¤§å°ä¸º0æˆ–1ï¼Œå…¶å†…éƒ¨è¾¹æ•°ä¸º0ï¼Œè´¡çŒ®ä¹Ÿä¸º0ï¼Œå¯è·³è¿‡
            if n_k <= 1:
                continue

            total_n_k += n_k

            # è®¡ç®—ç°‡å†…è¾¹æ•° L_in_k
            l_in_k = self._calculate_L_in(cluster_nodes)

            # è®¡ç®—ç°‡å¤–è¾¹æ•° L_out_k
            l_out_k = self._calculate_L_out(cluster_nodes)

            # è®¡ç®—ç°‡å†…å¯†åº¦é¡¹: 2*L_in_k / (n_k^2)
            term1 = (2 * l_in_k) / (n_k**2)

            # è®¡ç®—ç°‡é—´å¯†åº¦é¡¹: L_out_k / (n_k * (N - n_k))
            # N æ˜¯æ€»èŠ‚ç‚¹æ•°, (N - n_k) æ˜¯ç°‡å¤–èŠ‚ç‚¹æ•°
            denominator_term2 = n_k * (self.num_nodes - n_k)
            term2 = l_out_k / denominator_term2 if denominator_term2 != 0 else 0.0

            # ç´¯åŠ å„é¡¹
            sum_terms += n_k * (term1 - term2)

        # é¿å…é™¤ä»¥é›¶çš„æƒ…å†µ
        if total_n_k == 0:
            return 0.0

        nor_ti = sum_terms / total_n_k
        return nor_ti

    def get_all_metrics(self):
        """
        è®¡ç®—å¹¶è¿”å›æ‰€æœ‰å†…éƒ¨è¯„ä¼°æŒ‡æ ‡
        """
        _round = 4
        metrics = {
            "NorHo": round(self.normalized_homogeneity(), _round),
            "NorTi": round(self.normalized_tightness(), _round),
        }
        return metrics


class externalEvaluator:
    def __init__(self, cluster_labels, reference_labels):
        """
        èšç±»è¯„ä¼°ç±»ï¼šæ¯”è¾ƒèšç±»ç»“æœå’Œå‚è€ƒæ ‡ç­¾çš„å¤–éƒ¨ä¸€è‡´æ€§æŒ‡æ ‡
        """
        self.cluster_labels = np.array(cluster_labels)

        self.reference_labels = np.array(reference_labels)

        if len(self.cluster_labels) != len(self.reference_labels):
            raise ValueError("èšç±»ç»“æœä¸å‚è€ƒæ ‡ç­¾é•¿åº¦å¿…é¡»ä¸€è‡´")

        self.m = len(self.cluster_labels)
        # é¢„è®¡ç®—ç°‡é›†åˆ
        self.pred_clusters = {
            label: set(np.where(self.cluster_labels == label)[0])
            for label in np.unique(self.cluster_labels)
        }
        self.ref_clusters = {
            label: set(np.where(self.reference_labels == label)[0])
            for label in np.unique(self.reference_labels)
        }

    def _pairwise_counts(self):
        """è®¡ç®— a,b,c,d åˆ—è”è¡¨å‚æ•°"""
        a = b = c = d = 0
        for i, j in combinations(range(self.m), 2):
            same_cluster = self.cluster_labels[i] == self.cluster_labels[j]
            same_ref = self.reference_labels[i] == self.reference_labels[j]
            if same_cluster and same_ref:
                a += 1
            elif same_cluster and not same_ref:
                b += 1
            elif not same_cluster and same_ref:
                c += 1
            else:
                d += 1
        return a, b, c, d

    def micro_f1(self):
        """
        è®¡ç®—èšç±»å¤–éƒ¨è¯„ä¼°çš„å¾®F1åˆ†æ•°ï¼ˆMicro-F1 Scoreï¼‰

        æ ¸å¿ƒé€»è¾‘ï¼š
            1. ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ï¼šèšç±»åˆ¤å®šä¸ºâ€œåŒç°‡â€çš„æ ·æœ¬å¯¹ä¸­ï¼ŒçœŸå®â€œåŒæ ‡â€çš„æ¯”ä¾‹ â†’ a/(a+b)
            2. å¬å›ç‡ï¼ˆRecallï¼‰ï¼šçœŸå®â€œåŒæ ‡â€çš„æ ·æœ¬å¯¹ä¸­ï¼Œèšç±»åˆ¤å®šä¸ºâ€œåŒç°‡â€çš„æ¯”ä¾‹ â†’ a/(a+c)
            3. å¾®F1ï¼šç²¾ç¡®ç‡ä¸å¬å›ç‡çš„è°ƒå’Œå¹³å‡ â†’ 2*(P*R)/(P+R)

        Returns:
            float: å¾®F1åˆ†æ•°ï¼ˆèŒƒå›´0~1ï¼Œåˆ†æ•°è¶Šé«˜ï¼Œèšç±»ä¸å‚è€ƒæ ‡ç­¾ä¸€è‡´æ€§è¶Šå¥½ï¼‰

        å¼‚å¸¸å¤„ç†ï¼š
            è‹¥ç²¾ç¡®ç‡å’Œå¬å›ç‡å‡ä¸º0ï¼ˆæ— æœ‰æ•ˆåŒç°‡/åŒæ ‡å¯¹ï¼‰ï¼Œè¿”å›0é¿å…é™¤ä»¥é›¶
        """
        # 1. è·å–åˆ—è”è¡¨å‚æ•°
        a, b, c, _ = self._pairwise_counts()  # dä¸å‚ä¸å¾®F1è®¡ç®—ï¼Œç”¨_å¿½ç•¥

        # 2. è®¡ç®—å…¨å±€ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰å’Œå¬å›ç‡ï¼ˆRecallï¼‰
        # é¿å…é™¤ä»¥é›¶ï¼šè‹¥èšç±»æ— åŒç°‡å¯¹ï¼ˆa+b=0ï¼‰ï¼Œç²¾ç¡®ç‡ä¸º0ï¼›è‹¥å‚è€ƒæ— åŒæ ‡å¯¹ï¼ˆa+c=0ï¼‰ï¼Œå¬å›ç‡ä¸º0
        precision = a / (a + b) if (a + b) != 0 else 0.0
        recall = a / (a + c) if (a + c) != 0 else 0.0

        # 3. è®¡ç®—å¾®F1ï¼ˆè°ƒå’Œå¹³å‡ï¼‰
        if precision + recall == 0:
            return 0.0  # æ— æœ‰æ•ˆé¢„æµ‹æ—¶è¿”å›0
        micro_f1_score = 2 * (precision * recall) / (precision + recall)

        return micro_f1_score  # ä¿ç•™4ä½å°æ•°ï¼Œä¾¿äºç»“æœè§£è¯»

    def precision(self, i, j):
        """æŒ‰å…¬å¼è®¡ç®—çœŸå®ç°‡ i å’Œé¢„æµ‹ç°‡ j çš„ Precision"""
        ref_set = self.ref_clusters[i]
        pred_set = self.pred_clusters[j]
        if len(ref_set) == 0:
            return 0
        return len(ref_set & pred_set) / len(ref_set)

    def recall(self, i, j):
        """æŒ‰å…¬å¼è®¡ç®—çœŸå®ç°‡ i å’Œé¢„æµ‹ç°‡ j çš„ Recall"""
        ref_set = self.ref_clusters[i]
        pred_set = self.pred_clusters[j]
        if len(pred_set) == 0:
            return 0
        return len(ref_set & pred_set) / len(pred_set)

    def f1_score(self):
        """è¿”å›æ‰€æœ‰ç°‡ç»„åˆçš„ F1-score ç´¯åŠ å€¼"""
        total_f1 = 0
        for i in self.ref_clusters.keys():
            for j in self.pred_clusters.keys():
                p = self.precision(i, j)
                r = self.recall(i, j)
                if p + r > 0:
                    total_f1 += 2 * p * r / (p + r)
        return total_f1

    def accuracy(self):
        """èšç±»å‡†ç¡®ç‡ ACCï¼ŒåŸºäºæœ€ä¼˜æ˜ å°„"""
        y_true = self.reference_labels
        y_pred = self.cluster_labels
        D = max(y_pred.max(), y_true.max()) + 1
        # æ„å»ºæ··æ·†çŸ©é˜µ
        w = np.zeros((D, D), dtype=np.int64)
        for i in range(len(y_pred)):
            w[y_pred[i], y_true[i]] += 1
        # æ±‚æœ€å¤§åŒ¹é…
        row_ind, col_ind = linear_sum_assignment(w.max() - w)
        acc = w[row_ind, col_ind].sum() / len(y_pred)
        return acc

    def jaccard_coefficient(self):
        """Jaccard ç³»æ•°"""
        a, b, c, _ = self._pairwise_counts()
        return a / (a + b + c) if (a + b + c) else 0

    def fowlkes_mallows_index(self):
        """Fowlkes-Mallows æŒ‡æ•° (sklearn)"""
        return fowlkes_mallows_score(self.reference_labels, self.cluster_labels)

    def rand_index(self):
        """Rand æŒ‡æ•°"""
        a, _, _, d = self._pairwise_counts()
        return (a + d) / comb(self.m, 2)

    def adjusted_rand_index(self):
        """è°ƒæ•´åçš„ Rand æŒ‡æ•°"""
        return adjusted_rand_score(self.reference_labels, self.cluster_labels)

    def normalized_mutual_information(self):
        """å½’ä¸€åŒ–äº’ä¿¡æ¯"""
        return normalized_mutual_info_score(self.reference_labels, self.cluster_labels)

    def homogeneity_completeness_vmeasure(self):
        """åŒè´¨æ€§ã€å®Œæ•´æ€§ã€V-measure"""
        return homogeneity_completeness_v_measure(
            self.reference_labels, self.cluster_labels
        )

    def get_all_metrics(self):
        """è¿”å›å…¨éƒ¨æŒ‡æ ‡ç»“æœ"""
        h, c, v = self.homogeneity_completeness_vmeasure()
        a, b, c2, d = self._pairwise_counts()
        _round = 4
        return {
            "ACC": round(self.accuracy(), _round),  # å‡†ç¡®ç‡
            "JC": round(self.jaccard_coefficient(), _round),  # Jaccard ç³»æ•°
            "FMI": round(self.fowlkes_mallows_index(), _round),  # Fowlkes-Mallows æŒ‡æ•°
            "RI": round(self.rand_index(), _round),  # Rand æŒ‡æ•°
            "ARI": round(
                self.adjusted_rand_index(), _round
            ),  # è°ƒæ•´åçš„ Rand æŒ‡æ•°ï¼Œ0è¡¨ç¤ºå®Œå…¨éšæœº
            "NMI": round(self.normalized_mutual_information(), _round),  # å½’ä¸€åŒ–äº’ä¿¡æ¯
            "Micro-F1": round(self.micro_f1(), _round),
            # "SS": a,  # é¢„æµ‹ä¸çœŸå®å‡ä¸ºæ­£çš„æ ·æœ¬å¯¹æ•°
            # "SD": b,  # é¢„æµ‹ä¸ºæ­£è€ŒçœŸå®ä¸ºè´Ÿçš„æ ·æœ¬å¯¹æ•°
            # "DS": c2,  # é¢„æµ‹ä¸ºè´Ÿè€ŒçœŸå®ä¸ºæ­£çš„æ ·æœ¬å¯¹æ•°
            # "DD": d,  # é¢„æµ‹ä¸çœŸå®å‡ä¸ºè´Ÿçš„æ ·æœ¬å¯¹æ•°
        }

    def print_metrics(self):
        """æ‰“å°ç»“æœ"""
        metrics = self.get_all_metrics()
        print("\n=== èšç±»å¤–éƒ¨æŒ‡æ ‡è¯„ä¼°ç»“æœ ===")
        for k, v in metrics.items():
            print(f"{k:25s}: {v:.4f}" if isinstance(v, float) else f"{k:25s}: {v}")


if __name__ == "__main__":
    import pandas as pd

    cluster_labels = pd.read_csv(r"results\pred_and_real\citeseer_20251111_0804.csv")[
        "predict"
    ].values
    edges = pd.read_csv(r"stgraphs\citeseer.edges", header=None, sep=",")
    features = json2featmat("citeseer").toarray()

    ie = internalEvaluator(cluster_labels, edges.values, features)
    print(ie.normalized_homogeneity())
