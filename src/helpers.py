"""
Author: PumpkinğŸƒ
Date:2025-10-28
Description:helper functions
    class GraphAnalysis: å›¾æ•°æ®æ¢ç´¢åˆ†æå·¥å…·ç±»
    function compute_communititude_metrice: è®¡ç®—ç¤¾åŒºæŒ‡æ ‡
    function create_mapping: åˆ›å»ºèŠ‚ç‚¹ç´¢å¼•æ˜ å°„
"""

import os
import json
import joblib
import time
import shutil
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import sparse
import networkx as nx
from sklearn.decomposition import NMF
from collections import Counter, defaultdict

import warnings

warnings.filterwarnings("ignore")

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams["font.sans-serif"] = ["SimHei"]  # æŒ‡å®šé»˜è®¤å­—ä½“
plt.rcParams["axes.unicode_minus"] = False  # è§£å†³ä¿å­˜å›¾åƒæ—¶è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜


class GraphAnalysis:
    def __init__(self, adjacency_matrix):
        """
        ä¼˜åŒ–ç‰ˆç¨€ç–å›¾åˆ†æ

        å‚æ•°:
        adjacency_matrix: np.array æˆ– scipy.sparseçŸ©é˜µ, å›¾çš„é‚»æ¥çŸ©é˜µ
        """
        # è½¬æ¢ä¸ºç¨€ç–çŸ©é˜µæ ¼å¼ä»¥èŠ‚çœå†…å­˜
        if sparse.issparse(adjacency_matrix):
            self.adj_matrix = adjacency_matrix
        else:
            self.adj_matrix = sparse.csr_matrix(adjacency_matrix)

        self.n_nodes = self.adj_matrix.shape[0]

        # å¯¹äºå¤§å›¾ï¼Œä¸ç«‹å³åˆ›å»ºnetworkxå›¾å¯¹è±¡
        self.G = None

    def basic_statistics(self):
        """ä¼˜åŒ–çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯è®¡ç®—"""
        print("=" * 50)
        print("å›¾çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ (ä¼˜åŒ–ç‰ˆ)")
        print("=" * 50)

        start_time = time.time()

        # ä½¿ç”¨ç¨€ç–çŸ©é˜µæ“ä½œè®¡ç®—è¾¹æ•°
        n_edges = self.adj_matrix.nnz // 2  # æ— å‘å›¾

        # å›¾å¯†åº¦
        max_possible_edges = self.n_nodes * (self.n_nodes - 1) / 2
        density = n_edges / max_possible_edges if max_possible_edges > 0 else 0

        # æ£€æŸ¥æ˜¯å¦å¯¹ç§°ï¼ˆæ— å‘å›¾ï¼‰
        if sparse.issparse(self.adj_matrix):
            is_symmetric = (self.adj_matrix != self.adj_matrix.T).nnz == 0
        else:
            is_symmetric = np.allclose(self.adj_matrix, self.adj_matrix.T)

        elapsed_time = time.time() - start_time

        print(f"èŠ‚ç‚¹æ•°é‡: {self.n_nodes:,}")
        print(f"è¾¹æ•°é‡: {n_edges:,}")
        print(f"å›¾å¯†åº¦: {density:.6f}")
        print(f"å›¾ç±»å‹: {'æ— å‘å›¾' if is_symmetric else 'æœ‰å‘å›¾'}")
        print(f"è®¡ç®—æ—¶é—´: {elapsed_time:.2f}ç§’")
        print(f"ç¨€ç–åº¦: {(1-density)*100:.2f}%")

        return {
            "n_nodes": self.n_nodes,
            "n_edges": n_edges,
            "density": density,
            "is_directed": not is_symmetric,
            "sparsity": (1 - density),
        }

    def degree_analysis(self, sample_size=1000):
        """ä¼˜åŒ–çš„åº¦åˆ†å¸ƒåˆ†æï¼Œæ”¯æŒæŠ½æ ·"""
        print("\n" + "=" * 50)
        print("åº¦åˆ†å¸ƒåˆ†æ (ä¼˜åŒ–ç‰ˆ)")
        print("=" * 50)

        start_time = time.time()

        # ä½¿ç”¨ç¨€ç–çŸ©é˜µçš„å¿«é€Ÿåº¦è®¡ç®—
        if self._is_undirected():
            degrees = np.array(self.adj_matrix.sum(axis=1)).flatten()
        else:
            in_degrees = np.array(self.adj_matrix.sum(axis=0)).flatten()
            out_degrees = np.array(self.adj_matrix.sum(axis=1)).flatten()
            degrees = in_degrees + out_degrees

        # åŸºæœ¬ç»Ÿè®¡
        degree_stats = {
            "mean": np.mean(degrees),
            "std": np.std(degrees),
            "max": np.max(degrees),
            "min": np.min(degrees),
            "median": np.median(degrees),
        }

        print(f"å¹³å‡åº¦: {degree_stats['mean']:.2f}")
        print(f"åº¦æ ‡å‡†å·®: {degree_stats['std']:.2f}")
        print(f"æœ€å¤§åº¦: {degree_stats['max']}")
        print(f"æœ€å°åº¦: {degree_stats['min']}")
        print(f"åº¦ä¸­ä½æ•°: {degree_stats['median']}")

        # åº¦åˆ†å¸ƒæŠ½æ ·åˆ†æ
        if self.n_nodes > sample_size:
            sampled_indices = np.random.choice(self.n_nodes, sample_size, replace=False)
            sampled_degrees = degrees[sampled_indices]
            print(f"\nåŸºäº {sample_size} ä¸ªèŠ‚ç‚¹çš„æŠ½æ ·åˆ†æ:")
            print(f"  æŠ½æ ·å¹³å‡åº¦: {np.mean(sampled_degrees):.2f}")
            print(f"  æŠ½æ ·åº¦æ ‡å‡†å·®: {np.std(sampled_degrees):.2f}")

        elapsed_time = time.time() - start_time
        print(f"è®¡ç®—æ—¶é—´: {elapsed_time:.2f}ç§’")

        return degrees, degree_stats

    def _is_undirected(self):
        """æ£€æŸ¥å›¾æ˜¯å¦æ— å‘"""
        if sparse.issparse(self.adj_matrix):
            return (self.adj_matrix != self.adj_matrix.T).nnz == 0
        else:
            return np.allclose(self.adj_matrix, self.adj_matrix.T)

    def connected_components_analysis(self, max_components=10):
        """ä¼˜åŒ–çš„è¿é€šåˆ†é‡åˆ†æ"""
        print("\n" + "=" * 50)
        print("è¿é€šåˆ†é‡åˆ†æ (ä¼˜åŒ–ç‰ˆ)")
        print("=" * 50)

        if not self._is_undirected():
            print("æœ‰å‘å›¾çš„è¿é€šæ€§åˆ†æè¾ƒä¸ºå¤æ‚ï¼Œæ­¤å¤„çœç•¥")
            return None, None, None

        start_time = time.time()

        # ä½¿ç”¨scipyçš„è¿é€šåˆ†é‡ç®—æ³•ï¼ˆæ¯”è‡ªå®šä¹‰BFSå¿«å¾—å¤šï¼‰
        n_components, labels = sparse.csgraph.connected_components(
            self.adj_matrix, directed=False, return_labels=True
        )

        # è®¡ç®—å„è¿é€šåˆ†é‡å¤§å°
        component_sizes = Counter(labels)
        sorted_components = sorted(
            component_sizes.items(), key=lambda x: x[1], reverse=True
        )

        print(f"è¿é€šåˆ†é‡æ•°é‡: {n_components}")
        print(f"æœ€å¤§è¿é€šåˆ†é‡å¤§å°: {sorted_components[0][1]}")
        print(f"è¿é€šåˆ†é‡å¤§å°åˆ†å¸ƒ (å‰{min(max_components, n_components)}ä¸ª):")

        total_shown = 0
        for comp_id, size in sorted_components[:max_components]:
            print(f"  åˆ†é‡ {comp_id}: {size} ä¸ªèŠ‚ç‚¹")
            total_shown += size

        if n_components > max_components:
            remaining_nodes = self.n_nodes - total_shown
            print(
                f"  å…¶ä»– {n_components - max_components} ä¸ªåˆ†é‡: {remaining_nodes} ä¸ªèŠ‚ç‚¹"
            )

        # æ£€æŸ¥æ˜¯å¦è¿é€š
        is_connected = n_components == 1
        print(f"å›¾æ˜¯å¦è¿é€š: {is_connected}")

        elapsed_time = time.time() - start_time
        print(f"è®¡ç®—æ—¶é—´: {elapsed_time:.2f}ç§’")

        return n_components, labels, is_connected

    def centrality_analysis(self, top_k=10):
        """ä¼˜åŒ–çš„ä¸­å¿ƒæ€§åˆ†æ"""
        print("\n" + "=" * 50)
        print("ä¸­å¿ƒæ€§åˆ†æ (ä¼˜åŒ–ç‰ˆ)")
        print("=" * 50)

        start_time = time.time()

        # åªè®¡ç®—åº¦ä¸­å¿ƒæ€§ï¼ˆå…¶ä»–ä¸­å¿ƒæ€§è®¡ç®—æˆæœ¬å¤ªé«˜ï¼‰
        degrees = np.array(self.adj_matrix.sum(axis=1)).flatten()
        degree_centrality = degrees / (self.n_nodes - 1)

        print(f"åº¦ä¸­å¿ƒæ€§æœ€é«˜çš„ {top_k} ä¸ªèŠ‚ç‚¹:")
        top_indices = np.argpartition(degree_centrality, -top_k)[-top_k:]
        top_indices = top_indices[np.argsort(degree_centrality[top_indices])[::-1]]

        for i, node in enumerate(top_indices):
            print(f"  {i+1:2d}. èŠ‚ç‚¹ {node:5d}: {degree_centrality[node]:.6f}")

        elapsed_time = time.time() - start_time
        print(f"è®¡ç®—æ—¶é—´: {elapsed_time:.2f}ç§’")

        return degree_centrality

    def sampling_based_clustering(self, sample_size=1000):
        """åŸºäºæŠ½æ ·çš„èšç±»ç³»æ•°åˆ†æ"""
        print("\n" + "=" * 50)
        print("èšç±»ç³»æ•°åˆ†æ (æŠ½æ ·ç‰ˆ)")
        print("=" * 50)

        start_time = time.time()

        # æŠ½æ ·è®¡ç®—èšç±»ç³»æ•°
        if sample_size > self.n_nodes:
            sample_size = self.n_nodes

        sampled_indices = np.random.choice(self.n_nodes, sample_size, replace=False)
        clustering_coeffs = []

        for i in sampled_indices:
            neighbors = self.adj_matrix[i].nonzero()[1]
            k = len(neighbors)

            if k < 2:
                clustering_coeffs.append(0.0)
            else:
                # è®¡ç®—é‚»å±…ä¹‹é—´çš„è¾¹æ•°
                edges_between_neighbors = 0
                # åªæ£€æŸ¥éƒ¨åˆ†é‚»å±…å¯¹ä»¥é¿å…ç»„åˆçˆ†ç‚¸
                max_pairs = min(1000, k * (k - 1) // 2)
                if k > 50:  # å¯¹äºé«˜åº¦æ•°èŠ‚ç‚¹ï¼Œè¿›ä¸€æ­¥æŠ½æ ·
                    neighbor_pairs = []
                    for _ in range(max_pairs):
                        u, v = np.random.choice(neighbors, 2, replace=False)
                        if u != v and self.adj_matrix[u, v] > 0:
                            edges_between_neighbors += 1
                    coeff = (2 * edges_between_neighbors) / max_pairs
                else:
                    for u_idx, u in enumerate(neighbors):
                        for v in neighbors[u_idx + 1 :]:
                            if self.adj_matrix[u, v] > 0:
                                edges_between_neighbors += 1
                    coeff = (2 * edges_between_neighbors) / (k * (k - 1))

                clustering_coeffs.append(coeff)

        clustering_coeffs = np.array(clustering_coeffs)

        print(f"åŸºäº {sample_size} ä¸ªèŠ‚ç‚¹çš„æŠ½æ ·ç»“æœ:")
        print(f"å¹³å‡èšç±»ç³»æ•°: {np.mean(clustering_coeffs):.6f}")
        print(f"èšç±»ç³»æ•°æ ‡å‡†å·®: {np.std(clustering_coeffs):.6f}")
        print(f"èšç±»ç³»æ•°ä¸­ä½æ•°: {np.median(clustering_coeffs):.6f}")

        elapsed_time = time.time() - start_time
        print(f"è®¡ç®—æ—¶é—´: {elapsed_time:.2f}ç§’")

        return clustering_coeffs

    def efficient_visualization(self, max_nodes=1000):
        """é’ˆå¯¹å¤§å›¾çš„ç®€åŒ–å¯è§†åŒ–"""
        print("\n" + "=" * 50)
        print("ç®€åŒ–å¯è§†åŒ–")
        print("=" * 50)

        # å¦‚æœå›¾å¤ªå¤§ï¼Œåªå¯è§†åŒ–æœ€å¤§è¿é€šåˆ†é‡æˆ–æŠ½æ ·
        n_components, labels, _ = self.connected_components_analysis(max_components=5)

        if n_components == 1 and self.n_nodes > max_nodes:
            print("å›¾å¤ªå¤§ï¼Œè¿›è¡ŒæŠ½æ ·å¯è§†åŒ–...")
            # éšæœºæŠ½æ ·èŠ‚ç‚¹
            sample_nodes = np.random.choice(self.n_nodes, max_nodes, replace=False)
            subgraph = self.adj_matrix[sample_nodes, :][:, sample_nodes]
            G = nx.from_scipy_sparse_array(subgraph)
        else:
            # ä½¿ç”¨æœ€å¤§è¿é€šåˆ†é‡
            component_sizes = Counter(labels)
            largest_component_id = max(component_sizes, key=component_sizes.get)
            nodes_in_largest = np.where(labels == largest_component_id)[0]

            if len(nodes_in_largest) > max_nodes:
                print(f"æœ€å¤§è¿é€šåˆ†é‡æœ‰ {len(nodes_in_largest)} ä¸ªèŠ‚ç‚¹ï¼Œè¿›è¡ŒæŠ½æ ·...")
                nodes_in_largest = np.random.choice(
                    nodes_in_largest, max_nodes, replace=False
                )

            subgraph = self.adj_matrix[nodes_in_largest, :][:, nodes_in_largest]
            G = nx.from_scipy_sparse_array(subgraph)

        plt.figure(figsize=(15, 5))

        # åº¦åˆ†å¸ƒç›´æ–¹å›¾
        plt.subplot(1, 3, 1)
        degrees = [d for _, d in G.degree()]
        plt.hist(degrees, bins=30, alpha=0.7, color="skyblue", edgecolor="black")
        plt.xlabel("åº¦")
        plt.ylabel("é¢‘ç‡")
        plt.title("åº¦åˆ†å¸ƒ")

        # å›¾ç»“æ„å¯è§†åŒ–
        plt.subplot(1, 3, 2)
        pos = nx.spring_layout(G, seed=42)
        nx.draw(G, pos, node_size=20, alpha=0.6, edge_color="gray", width=0.5)
        plt.title("å›¾ç»“æ„")

        # é‚»æ¥çŸ©é˜µçƒ­å›¾ï¼ˆåªæ˜¾ç¤ºéƒ¨åˆ†ï¼‰
        plt.subplot(1, 3, 3)
        if subgraph.shape[0] > 500:
            # å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼Œè¿›ä¸€æ­¥æŠ½æ ·
            sample_idx = np.random.choice(subgraph.shape[0], 500, replace=False)
            subgraph = subgraph[sample_idx, :][:, sample_idx]

        sns.heatmap(subgraph.toarray(), cmap="Blues", cbar=True)
        plt.title("é‚»æ¥çŸ©é˜µæŠ½æ ·")

        plt.tight_layout()
        plt.show()

    def memory_usage_report(self):
        """å†…å­˜ä½¿ç”¨æŠ¥å‘Š"""
        print("\n" + "=" * 50)
        print("å†…å­˜ä½¿ç”¨æŠ¥å‘Š")
        print("=" * 50)

        if sparse.issparse(self.adj_matrix):
            dense_size = self.n_nodes * self.n_nodes * 8 / (1024**3)  # GB
            sparse_size = (
                self.adj_matrix.data.nbytes
                + self.adj_matrix.indices.nbytes
                + self.adj_matrix.indptr.nbytes
            ) / (
                1024**3
            )  # GB

            print(f"ç¨ å¯†çŸ©é˜µä¼°è®¡å¤§å°: {dense_size:.2f} GB")
            print(f"ç¨€ç–çŸ©é˜µå®é™…å¤§å°: {sparse_size:.2f} GB")
            print(f"å†…å­˜èŠ‚çœ: {(1 - sparse_size/dense_size)*100:.1f}%")

    def comprehensive_analysis(self, visualize=True, sample_size=1000):
        """æ‰§è¡Œå…¨é¢çš„ä¼˜åŒ–åˆ†æ"""
        print("å¼€å§‹ç¨€ç–å›¾æ¢ç´¢æ€§åˆ†æ (ä¼˜åŒ–ç‰ˆ)...")

        # å†…å­˜æŠ¥å‘Š
        self.memory_usage_report()

        # åŸºæœ¬ç»Ÿè®¡
        basic_stats = self.basic_statistics()

        # åº¦åˆ†æ
        degrees, degree_stats = self.degree_analysis(sample_size)

        # è¿é€šæ€§åˆ†æ
        connectivity_results = self.connected_components_analysis()

        # ä¸­å¿ƒæ€§åˆ†æ
        centrality = self.centrality_analysis()

        # èšç±»ç³»æ•°åˆ†æï¼ˆæŠ½æ ·ï¼‰
        clustering = self.sampling_based_clustering(sample_size)

        # ç®€åŒ–å¯è§†åŒ–
        if visualize and self.n_nodes <= 10000:  # åªåœ¨èŠ‚ç‚¹æ•°é€‚ä¸­æ—¶å¯è§†åŒ–
            self.efficient_visualization()
        elif visualize:
            print("\nå›¾å¤ªå¤§ï¼Œè·³è¿‡è¯¦ç»†å¯è§†åŒ–")
            if input("æ˜¯å¦æ˜¾ç¤ºç®€åŒ–æŠ½æ ·å¯è§†åŒ–? (y/n): ").lower() == "y":
                self.efficient_visualization()

        # è¿”å›æ‰€æœ‰åˆ†æç»“æœ
        return {
            "basic_stats": basic_stats,
            "degrees": degrees,
            "degree_stats": degree_stats,
            "connectivity": connectivity_results,
            "centrality": centrality,
            "clustering": clustering,
        }


def compute_communitude_metric(A, labels, axis=0):
    """
    Calculate the communitude metric for each community to compare intra-layer and inter-layer community quality.
    """
    A = np.array(A)
    labels = np.array(labels)
    total_edge_weight = np.sum(A)
    unique_communities = np.unique(labels)
    results = {}

    for ck in unique_communities:
        if axis == 0:
            rows_in_ck = np.where(labels == ck)[0]
            submatrix = A[np.ix_(rows_in_ck, list(range(A.shape[1])))]
            e_intra_ck = np.sum(submatrix)
            e_inter_ck = np.sum(A[rows_in_ck, :]) - e_intra_ck
        else:
            cols_in_ck = np.where(labels == ck)[0]
            submatrix = A[np.ix_(list(range(A.shape[0])), cols_in_ck)]
            e_intra_ck = np.sum(submatrix)
            e_inter_ck = np.sum(A[:, cols_in_ck]) - e_intra_ck

        if total_edge_weight == 0:
            results[ck] = 0.0
            continue

        numerator = (e_intra_ck / total_edge_weight) - (
            (e_intra_ck + e_inter_ck) / (2 * total_edge_weight)
        ) ** 2
        denominator = ((e_intra_ck + e_inter_ck) / (2 * total_edge_weight)) ** 2 * (
            1 - ((e_intra_ck + e_inter_ck) / (2 * total_edge_weight)) ** 2
        )

        results[ck] = 0.0 if denominator == 0 else numerator / denominator

    return results


def create_mapping(row):
    if row["type"] == "intra":
        return 100 + row["community_id"]
    else:
        return 200 + row["community_id"]


def json2long(json_input_path, long_output_path):
    """
    å°† JSON {"èŠ‚ç‚¹ID": [æ‹¥æœ‰çš„å±æ€§IDåˆ—è¡¨]} è½¬æ¢ä¸ºé•¿æ ¼å¼ã€‚
    è¾“å‡ºæ ¼å¼ï¼šæ¯è¡Œ "èŠ‚ç‚¹ID\tå±æ€§ID"ã€‚
    æ’åºæ–¹å¼ï¼šé¦–å…ˆæŒ‰å±æ€§IDå‡åºï¼Œç„¶åæŒ‰èŠ‚ç‚¹IDå‡åºã€‚

    :param json_input_path: è¾“å…¥çš„ JSON æ–‡ä»¶è·¯å¾„ã€‚
    :param long_output_path: è¾“å‡ºçš„é•¿æ ¼å¼æ–‡ä»¶è·¯å¾„ã€‚
    """
    print(f"[*] å¼€å§‹è½¬æ¢: {json_input_path} -> {long_output_path}")

    # 1. è¯»å– JSON æ–‡ä»¶
    try:
        with open(json_input_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"[!] é”™è¯¯: è¾“å…¥æ–‡ä»¶ '{json_input_path}' æœªæ‰¾åˆ°ã€‚")
        return
    except json.JSONDecodeError:
        print(f"[!] é”™è¯¯: è¾“å…¥æ–‡ä»¶ '{json_input_path}' ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚")
        return

    # 2. æ•°æ®é¢„å¤„ç†å’Œæ”¶é›†
    # åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œkeyä¸ºå±æ€§IDï¼Œvalueä¸ºæ‹¥æœ‰è¯¥å±æ€§çš„èŠ‚ç‚¹IDåˆ—è¡¨
    attr_to_nodes = defaultdict(list)

    all_node_ids = set()
    all_attr_ids = set()

    for node_id_str, attr_id_list in data.items():
        node_id_int = int(node_id_str)
        all_node_ids.add(node_id_int)

        for attr_id in attr_id_list:
            attr_id_int = int(attr_id)
            attr_to_nodes[attr_id_int].append(node_id_int)
            all_attr_ids.add(attr_id_int)

    # 3. æ’åº
    # å¯¹æ‰€æœ‰å±æ€§IDè¿›è¡Œå‡åºæ’åº
    sorted_attr_ids = sorted(list(all_attr_ids))

    # å¯¹æ¯ä¸ªå±æ€§å¯¹åº”çš„èŠ‚ç‚¹IDåˆ—è¡¨è¿›è¡Œå‡åºæ’åº
    for attr_id in sorted_attr_ids:
        attr_to_nodes[attr_id].sort()

    print(f"[*] å‘ç° {len(all_node_ids)} ä¸ªèŠ‚ç‚¹, {len(all_attr_ids)} ä¸ªç‹¬ç‰¹å±æ€§ã€‚")

    # 4. å†™å…¥é•¿æ ¼å¼æ–‡ä»¶
    total_lines = 0
    try:
        with open(long_output_path, "w", encoding="utf-8") as f:
            # éå†æ’åºåçš„å±æ€§ID
            for attr_id in sorted_attr_ids:
                # éå†å½“å‰å±æ€§ä¸‹æ’åºåçš„èŠ‚ç‚¹IDåˆ—è¡¨
                for node_id in attr_to_nodes[attr_id]:
                    f.write(f"{node_id}\t{attr_id}\n")
                    total_lines += 1
        print(
            f"[âœ“] è½¬æ¢æˆåŠŸ! å…±ç”Ÿæˆ {total_lines} æ¡è®°å½•ï¼Œå·²ä¿å­˜åˆ° '{long_output_path}'"
        )

    except IOError as e:
        print(f"[!] é”™è¯¯: å†™å…¥æ–‡ä»¶ '{long_output_path}' å¤±è´¥ã€‚ {e}")


def compute_AS_with_NMF(A, r, random_state=42):
    """
    ä½¿ç”¨éè´ŸçŸ©é˜µåˆ†è§£(NMF)è®¡ç®—ç»™å®šé‚»æ¥çŸ©é˜µ(æˆ–ç›¸ä¼¼åº¦çŸ©é˜µ)Açš„éå¯¹ç§°æƒŠå–œåº¦(Asymmetric Surprise, AS)ã€‚

    å‚æ•°:
        A (numpy.ndarray): å›¾çš„é‚»æ¥çŸ©é˜µï¼ˆæˆ–ç›¸ä¼¼åº¦çŸ©é˜µï¼‰ï¼Œå½¢çŠ¶ä¸º(nÃ—n)ï¼Œå…¶ä¸­næ˜¯èŠ‚ç‚¹æ•°
        r (int): è®¾å®šçš„è¦æ£€æµ‹çš„ç¤¾åŒºæ•°é‡
        random_state (int, optional): éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ï¼Œç”¨äºç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§ï¼Œé»˜è®¤ä¸º42

    è¿”å›:
        tuple: åŒ…å«ä¸¤ä¸ªå…ƒç´ çš„å…ƒç»„
            - float: è®¡ç®—å¾—åˆ°çš„éå¯¹ç§°æƒŠå–œåº¦(AS)å€¼
            - numpy.ndarray: æ¯ä¸ªèŠ‚ç‚¹æ‰€å±çš„ç¤¾åŒºæ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(n,)

    ç®—æ³•åŸç†:
        1. ä½¿ç”¨NMFå°†é‚»æ¥çŸ©é˜µåˆ†è§£ä¸ºä¸¤ä¸ªéè´ŸçŸ©é˜µçš„ä¹˜ç§¯ï¼Œä»è€Œè·å¾—èŠ‚ç‚¹çš„ç¤¾åŒºéš¶å±åº¦
        2. åŸºäºåˆ†è§£ç»“æœç¡®å®šæ¯ä¸ªèŠ‚ç‚¹çš„ç¤¾åŒºæ ‡ç­¾
        3. è®¡ç®—å®é™…çš„ç¤¾åŒºå†…è¾¹æ¯”ä¾‹ä¸éšæœºåˆ†å¸ƒä¸‹çš„æœŸæœ›ç¤¾åŒºå†…è¾¹æ¯”ä¾‹
        4. ä½¿ç”¨KLæ•£åº¦è®¡ç®—è¿™ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œå¾—åˆ°éå¯¹ç§°æƒŠå–œåº¦
    """
    # è·å–å›¾ä¸­èŠ‚ç‚¹çš„æ•°é‡
    n = A.shape[0]

    # -------- Step 1: NMF Decomposition --------
    # åˆå§‹åŒ–NMFæ¨¡å‹ï¼Œä½¿ç”¨NNDSVD(éè´ŸåŒé‡å¥‡å¼‚å€¼åˆ†è§£)ä½œä¸ºåˆå§‹åŒ–æ–¹æ³•
    # è®¾ç½®æœ€å¤§è¿­ä»£æ¬¡æ•°ä¸º500ä»¥ç¡®ä¿æ”¶æ•›
    nmf = NMF(n_components=r, init="nndsvd", random_state=random_state, max_iter=500)

    # å¯¹é‚»æ¥çŸ©é˜µAè¿›è¡ŒNMFåˆ†è§£ï¼Œå¾—åˆ°èŠ‚ç‚¹-ç¤¾åŒºéš¶å±åº¦çŸ©é˜µU(nÃ—k)
    U = nmf.fit_transform(A)  # n x k

    # å¯¹æ¯ä¸ªèŠ‚ç‚¹ï¼Œé€‰æ‹©éš¶å±åº¦æœ€å¤§çš„ç¤¾åŒºä½œä¸ºå…¶æ ‡ç­¾
    labels = np.argmax(U, axis=1)  # å–æœ€å¤§éš¶å±åº¦çš„ç¤¾åŒºä½œä¸ºæ ‡ç­¾

    # -------- Step 2: compute community structure --------
    # è®¡ç®—å›¾ä¸­è¾¹çš„æ€»æ•°ï¼ˆå› ä¸ºé‚»æ¥çŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼Œæ‰€ä»¥éœ€è¦é™¤ä»¥2ï¼‰
    E = np.sum(A) / 2  # total number of edges

    # è®¡ç®—ç¤¾åŒºå†…éƒ¨å®é™…å­˜åœ¨çš„è¾¹æ•°
    E_intra = 0
    for c in set(labels):
        # è·å–å±äºå½“å‰ç¤¾åŒºcçš„æ‰€æœ‰èŠ‚ç‚¹çš„ç´¢å¼•
        idx = np.where(labels == c)[0]
        # æå–è¿™äº›èŠ‚ç‚¹ç»„æˆçš„å­å›¾çš„é‚»æ¥çŸ©é˜µ
        subgraph = A[np.ix_(idx, idx)]
        # ç´¯åŠ å­å›¾ä¸­çš„è¾¹æ•°ï¼ˆåŒæ ·é™¤ä»¥2é¿å…é‡å¤è®¡ç®—ï¼‰
        E_intra += np.sum(subgraph) / 2

    # è®¡ç®—å®é™…çš„ç¤¾åŒºå†…è¾¹æ¯”ä¾‹q
    q = E_intra / E if E > 0 else 0

    # è®¡ç®—éšæœºåˆ†å¸ƒä¸‹æœŸæœ›çš„ç¤¾åŒºå†…è¾¹æ¯”ä¾‹q_exp
    # é¦–å…ˆè®¡ç®—æ¯ä¸ªç¤¾åŒºçš„å¤§å°
    sizes = [np.sum(labels == c) for c in set(labels)]
    # è®¡ç®—æœŸæœ›çš„ç¤¾åŒºå†…è¾¹æ¯”ä¾‹ï¼šæ‰€æœ‰ç¤¾åŒºå¯èƒ½çš„å†…éƒ¨è¾¹æ•°ä¹‹å’Œé™¤ä»¥å›¾ä¸­å¯èƒ½çš„æ€»è¾¹æ•°
    q_exp = sum(s * (s - 1) / 2 for s in sizes) / (n * (n - 1) / 2)

    # å¤„ç†è¾¹ç•Œæƒ…å†µï¼šå¦‚æœqæˆ–q_expä¸º0æˆ–1ï¼Œåˆ™KLæ•£åº¦æ— æ³•å®šä¹‰ï¼Œè¿”å›0
    if q in [0, 1] or q_exp in [0, 1]:
        return 0.0, labels

    # è®¡ç®—KLæ•£åº¦ï¼šè¡¡é‡å®é™…åˆ†å¸ƒä¸æœŸæœ›åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚
    KL = q * np.log(q / q_exp) + (1 - q) * np.log((1 - q) / (1 - q_exp))

    # è®¡ç®—éå¯¹ç§°æƒŠå–œåº¦ï¼šKLæ•£åº¦ä¹˜ä»¥2å€çš„è¾¹æ•°
    AS = 2 * E * KL

    # è¿”å›è®¡ç®—å¾—åˆ°çš„éå¯¹ç§°æƒŠå–œåº¦å’ŒèŠ‚ç‚¹ç¤¾åŒºæ ‡ç­¾
    return AS, labels


def standardize_feature_ids(graphs_dir, output_dir="st"):
    """
    æ£€æµ‹ç‰¹å¾IDä¸è¿ç»­æˆ–ä¸ä»0å¼€å§‹çš„æ–‡ä»¶ï¼Œå¹¶å°†å…¶æ ‡å‡†åŒ–ï¼ˆä»0å¼€å§‹ï¼Œè¿ç»­åŒ–ï¼‰

    Args:
        graphs_dir (str): graphsç›®å½•è·¯å¾„
        output_dir (str): è¾“å‡ºç›®å½•åç§°ï¼Œé»˜è®¤ä¸º'st'

    Returns:
        dict: å¤„ç†ç»“æœ
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    results = {}

    # éå†graphsç›®å½•ä¸‹çš„æ‰€æœ‰.featuresæ–‡ä»¶
    for filename in os.listdir(graphs_dir):
        if filename.endswith(".features"):
            file_path = os.path.join(graphs_dir, filename)

            try:
                # è¯»å–.featuresæ–‡ä»¶
                with open(file_path, "r") as f:
                    features_data = json.load(f)

                # æ”¶é›†æ‰€æœ‰ç‰¹å¾ID
                all_feature_ids = set()
                for node_features in features_data.values():
                    all_feature_ids.update(node_features)

                # æ£€æŸ¥ç‰¹å¾IDæ˜¯å¦è¿ç»­ä¸”ä»0å¼€å§‹
                all_feature_ids = sorted(list(all_feature_ids))
                expected_ids = list(range(len(all_feature_ids)))

                # å¦‚æœç‰¹å¾IDå·²ç»æ˜¯ä»0å¼€å§‹ä¸”è¿ç»­çš„ï¼Œåˆ™ç›´æ¥å¤åˆ¶æ–‡ä»¶
                if all_feature_ids == expected_ids:
                    # ç›´æ¥å¤åˆ¶æ–‡ä»¶åˆ°stç›®å½•
                    output_file_path = os.path.join(output_dir, filename)
                    shutil.copy(file_path, output_file_path)

                    # åˆ›å»ºèŠ‚ç‚¹IDæ˜ å°„æ–‡ä»¶ï¼ˆä¿æŒä¸å˜ï¼‰
                    node_mapping = {
                        node_id: node_id for node_id in features_data.keys()
                    }
                    mapping_file_path = os.path.join(
                        output_dir, filename.replace(".features", ".node_mapping.json")
                    )
                    with open(mapping_file_path, "w") as f:
                        json.dump(node_mapping, f, indent=2)

                    results[filename] = {
                        "status": "already_standardized",
                        "original_feature_count": len(all_feature_ids),
                        "mapping_file": mapping_file_path,
                    }
                else:
                    # éœ€è¦æ ‡å‡†åŒ–ç‰¹å¾ID
                    # åˆ›å»ºç‰¹å¾IDæ˜ å°„ï¼ˆåŸID -> æ–°IDï¼‰
                    feature_id_mapping = {
                        old_id: new_id for new_id, old_id in enumerate(all_feature_ids)
                    }

                    # åˆ›å»ºæ ‡å‡†åŒ–åçš„æ•°æ®
                    standardized_data = {}
                    node_mapping = {}  # èŠ‚ç‚¹IDæ˜ å°„ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰

                    for node_idx, (node_id, node_features) in enumerate(
                        features_data.items()
                    ):
                        # ä¿æŒèŠ‚ç‚¹IDä¸å˜ï¼Œåªæ ‡å‡†åŒ–ç‰¹å¾ID
                        standardized_features = [
                            feature_id_mapping[feat_id] for feat_id in node_features
                        ]
                        standardized_data[node_id] = standardized_features
                        node_mapping[node_id] = node_idx  # å¦‚æœéœ€è¦é‡æ–°ç¼–å·èŠ‚ç‚¹

                    # ä¿å­˜æ ‡å‡†åŒ–åçš„.featuresæ–‡ä»¶
                    output_file_path = os.path.join(output_dir, filename)
                    with open(output_file_path, "w") as f:
                        json.dump(standardized_data, f, indent=2)

                    # ä¿å­˜ç‰¹å¾IDæ˜ å°„å…³ç³»
                    mapping_info = {
                        "feature_id_mapping": feature_id_mapping,
                        "node_mapping": node_mapping,
                        "original_min_feature_id": (
                            min(all_feature_ids) if all_feature_ids else None
                        ),
                        "original_max_feature_id": (
                            max(all_feature_ids) if all_feature_ids else None
                        ),
                        "new_feature_count": len(all_feature_ids),
                    }

                    mapping_file_path = os.path.join(
                        output_dir, filename.replace(".features", ".mapping.json")
                    )
                    with open(mapping_file_path, "w") as f:
                        json.dump(mapping_info, f, indent=2)

                    results[filename] = {
                        "status": "standardized",
                        "original_feature_count": len(all_feature_ids),
                        "new_feature_count": len(all_feature_ids),
                        "min_feature_id": (
                            min(all_feature_ids) if all_feature_ids else None
                        ),
                        "max_feature_id": (
                            max(all_feature_ids) if all_feature_ids else None
                        ),
                        "output_file": output_file_path,
                        "mapping_file": mapping_file_path,
                    }

            except Exception as e:
                results[filename] = {"status": "error", "error": str(e)}

    return results


def determine_community_number(A, max_r=10):
    """
    ä½¿ç”¨éå¯¹ç§°æƒŠå–œåº¦(AS)æŒ‡æ ‡è‡ªåŠ¨ç¡®å®šå›¾çš„æœ€ä¼˜ç¤¾åŒºæ•°é‡ã€‚

    å‚æ•°:
        A (numpy.ndarray): å›¾çš„é‚»æ¥çŸ©é˜µï¼Œå½¢çŠ¶ä¸º(nÃ—n)ï¼Œå…¶ä¸­næ˜¯èŠ‚ç‚¹æ•°
        max_r (int, optional): å°è¯•çš„æœ€å¤§ç¤¾åŒºæ•°é‡ï¼Œé»˜è®¤å€¼ä¸º10

    è¿”å›:
        int: ä½¿ASå€¼æœ€å¤§çš„æœ€ä¼˜ç¤¾åŒºæ•°é‡

    ç®—æ³•åŸç†:
        1. éå†ä»2åˆ°max_rçš„æ‰€æœ‰å¯èƒ½ç¤¾åŒºæ•°é‡k
        2. å¯¹æ¯ä¸ªkå€¼ï¼Œä½¿ç”¨compute_AS_with_NMFè®¡ç®—å¯¹åº”çš„éå¯¹ç§°æƒŠå–œåº¦(AS)å€¼
        3. æ‰¾å‡ºASå€¼æœ€å¤§çš„kå€¼ï¼Œä½œä¸ºæœ€ä¼˜ç¤¾åŒºæ•°é‡

    æ³¨æ„äº‹é¡¹:        - ç¤¾åŒºæ•°é‡é€šå¸¸ä»2å¼€å§‹å°è¯•ï¼Œå› ä¸ºå•ä¸ªç¤¾åŒºæ²¡æœ‰åˆ’åˆ†æ„ä¹‰
        - å½“å­˜åœ¨å¤šä¸ªkå€¼å¯¹åº”ç›¸åŒçš„æœ€å¤§ASå€¼æ—¶ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå‡ºç°çš„kå€¼
    """
    # åˆå§‹åŒ–åˆ—è¡¨ç”¨äºå­˜å‚¨ä¸åŒç¤¾åŒºæ•°é‡å¯¹åº”çš„ASå€¼
    AS_values = []

    # éå†ä»2åˆ°max_rçš„æ‰€æœ‰å¯èƒ½ç¤¾åŒºæ•°é‡k
    # 2æ˜¯ç¤¾åŒºæ•°é‡çš„æœ€å°æœ‰æ„ä¹‰å€¼ï¼Œå•ä¸ªç¤¾åŒºæ— æ³•ä½“ç°ç¤¾åŒºåˆ’åˆ†
    for k in range(2, max_r + 1):
        # è®¡ç®—å½“å‰ç¤¾åŒºæ•°é‡kå¯¹åº”çš„éå¯¹ç§°æƒŠå–œåº¦å€¼
        # ä½¿ç”¨å›ºå®šçš„random_state=42ç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§
        AS_values.append(compute_AS_with_NMF(A, k, random_state=42))

    # æ‰¾å‡ºæ‰€æœ‰ASå€¼ä¸­çš„æœ€å¤§å€¼
    max_AS = max(AS_values)

    # éå†ASå€¼åˆ—è¡¨ï¼Œæ‰¾å‡ºç¬¬ä¸€ä¸ªç­‰äºæœ€å¤§å€¼çš„ç´¢å¼•
    for index, value in enumerate(AS_values):
        if value == max_AS:
            # å°†ç´¢å¼•è½¬æ¢ä¸ºå¯¹åº”çš„ç¤¾åŒºæ•°é‡kï¼ˆç´¢å¼•ä»0å¼€å§‹ï¼Œå¯¹åº”k=2ï¼‰
            r = index + 2
            # è¿”å›æœ€ä¼˜ç¤¾åŒºæ•°é‡
            return r


def check_featjson(featdict):
    # éªŒè¯èŠ‚ç‚¹idæ˜¯å¦ä»é›¶å¼€å§‹ä¸”è¿ç»­
    node_ids = sorted(list(int(node_id) for node_id in featdict.keys()))
    if not all(int(node_id) == i for i, node_id in enumerate(node_ids)):
        max_id = max(int(node_id) for node_id in node_ids)
        min_id = min(int(node_id) for node_id in node_ids)
        print(f"æœ€å¤§èŠ‚ç‚¹ID: {max_id}")
        print(f"æœ€å°èŠ‚ç‚¹ID: {min_id}")
        print(f"é¢„æœŸèŠ‚ç‚¹æ•°: {max_id - min_id + 1}, å®é™…èŠ‚ç‚¹æ•°: {len(node_ids)}")
    # éªŒè¯å±æ€§idæ˜¯å¦ä»é›¶å¼€å§‹ä¸”è¿ç»­
    all_feat = set()
    for node, features in featdict.items():
        all_feat.update(features)
    all_feat = sorted(list(all_feat))
    if not all(int(feat_id) == i for i, feat_id in enumerate(all_feat)):
        print(f"æœ€å¤§å±æ€§ID: {max(int(feat_id) for feat_id in all_feat)}")
        print(f"æœ€å°å±æ€§ID: {min(int(feat_id) for feat_id in all_feat)}")
        print(
            f"é¢„æœŸå±æ€§æ•°: {max(int(feat_id) for feat_id in all_feat) - min(int(feat_id) for feat_id in all_feat) + 1}, å®é™…å±æ€§æ•°: {len(all_feat)}"
        )
        raise ValueError("Feature IDs must be consecutive integers starting from 0")
    pass


def check_edges(edgesframe):
    # éªŒè¯æ˜¯å¦ä»é›¶å¼€å§‹ä¸”è¿ç»­
    edges = np.unique(np.array(edgesframe, dtype=int))
    max_id = max(edges.max(), edges.min())
    min_id = min(edges.max(), edges.min())
    if not all(
        int(node_id) == i for i, node_id in enumerate(range(min_id, max_id + 1))
    ):
        raise ValueError("Edge IDs must be consecutive integers starting from 0")


def json2featmat(file_path=None):
    with open(file_path, "r") as f:
        # è§£æJSONæ–‡ä»¶
        features_dict = json.load(f)

    # å‡†å¤‡æ•°æ®ç»“æ„
    row_indices = []  # è¡Œç´¢å¼•ï¼ˆèŠ‚ç‚¹IDï¼‰
    col_indices = []  # åˆ—ç´¢å¼•ï¼ˆç‰¹å¾IDï¼‰
    data = []  # æ•°æ®å€¼ï¼ˆè¿™é‡Œéƒ½æ˜¯1ï¼‰

    # è·å–æ‰€æœ‰èŠ‚ç‚¹å’Œç‰¹å¾
    nodes = list(features_dict.keys())

    # ç¡®å®šæœ€å¤§ç‰¹å¾ID
    max_feature_id = 0
    for node, features in features_dict.items():
        if features:
            current_max = max(features)
            if current_max > max_feature_id:
                max_feature_id = current_max

    # æ„å»ºç¨€ç–çŸ©é˜µæ•°æ®
    for node_idx, node in enumerate(nodes):
        for feature_id in features_dict[node]:
            row_indices.append(node_idx)
            col_indices.append(feature_id)
            data.append(1.0)  # å­˜åœ¨ç‰¹å¾å€¼ä¸º1

    # åˆ›å»ºç¨€ç–çŸ©é˜µï¼ˆCOOæ ¼å¼ï¼‰
    num_nodes = len(nodes)
    num_features = max_feature_id + 1

    coo = sparse.coo_matrix(
        (data, (row_indices, col_indices)), shape=(num_nodes, num_features)
    )
    csr = coo.tocsr()
    return csr


if __name__ == "__main__":
    # standardize_feature_ids('stgraphs')

    for file in os.listdir("stgraphs"):
        if file.endswith(".features"):
            dataname = file.split(".")[0]
            with open(f"stgraphs/{file}", "r") as f:
                print(f"checking {dataname}")
                features_dict = json.load(f)
            edgesframe = pd.read_csv(f"stgraphs/{dataname}.edges", header=None)
            check_edges(edgesframe)
            check_featjson(features_dict)
            # json2featmat(f"stgraphs/{file}")
