"""
Author: PumpkinğŸƒ
Date:2025-10-28
Description:helper functions
    class GraphAnalysis: å›¾æ•°æ®æ¢ç´¢åˆ†æå·¥å…·ç±»
    function compute_communititude_metrice: è®¡ç®—ç¤¾åŒºæŒ‡æ ‡
    function create_mapping: åˆ›å»ºèŠ‚ç‚¹ç´¢å¼•æ˜ å°„
"""

import os
import json
import joblib
import time

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import sparse
import networkx as nx
from collections import Counter

import warnings

warnings.filterwarnings("ignore")

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']  # æŒ‡å®šé»˜è®¤å­—ä½“
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³ä¿å­˜å›¾åƒæ—¶è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜

class GraphAnalysis:
    def __init__(self, adjacency_matrix):
        """
        ä¼˜åŒ–ç‰ˆç¨€ç–å›¾åˆ†æ

        å‚æ•°:
        adjacency_matrix: np.array æˆ– scipy.sparseçŸ©é˜µ, å›¾çš„é‚»æ¥çŸ©é˜µ
        """
        # è½¬æ¢ä¸ºç¨€ç–çŸ©é˜µæ ¼å¼ä»¥èŠ‚çœå†…å­˜
        if sparse.issparse(adjacency_matrix):
            self.adj_matrix = adjacency_matrix
        else:
            self.adj_matrix = sparse.csr_matrix(adjacency_matrix)

        self.n_nodes = self.adj_matrix.shape[0]

        # å¯¹äºå¤§å›¾ï¼Œä¸ç«‹å³åˆ›å»ºnetworkxå›¾å¯¹è±¡
        self.G = None

    def basic_statistics(self):
        """ä¼˜åŒ–çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯è®¡ç®—"""
        print("=" * 50)
        print("å›¾çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ (ä¼˜åŒ–ç‰ˆ)")
        print("=" * 50)

        start_time = time.time()

        # ä½¿ç”¨ç¨€ç–çŸ©é˜µæ“ä½œè®¡ç®—è¾¹æ•°
        n_edges = self.adj_matrix.nnz // 2  # æ— å‘å›¾

        # å›¾å¯†åº¦
        max_possible_edges = self.n_nodes * (self.n_nodes - 1) / 2
        density = n_edges / max_possible_edges if max_possible_edges > 0 else 0

        # æ£€æŸ¥æ˜¯å¦å¯¹ç§°ï¼ˆæ— å‘å›¾ï¼‰
        if sparse.issparse(self.adj_matrix):
            is_symmetric = (self.adj_matrix != self.adj_matrix.T).nnz == 0
        else:
            is_symmetric = np.allclose(self.adj_matrix, self.adj_matrix.T)

        elapsed_time = time.time() - start_time

        print(f"èŠ‚ç‚¹æ•°é‡: {self.n_nodes:,}")
        print(f"è¾¹æ•°é‡: {n_edges:,}")
        print(f"å›¾å¯†åº¦: {density:.6f}")
        print(f"å›¾ç±»å‹: {'æ— å‘å›¾' if is_symmetric else 'æœ‰å‘å›¾'}")
        print(f"è®¡ç®—æ—¶é—´: {elapsed_time:.2f}ç§’")
        print(f"ç¨€ç–åº¦: {(1-density)*100:.2f}%")

        return {
            "n_nodes": self.n_nodes,
            "n_edges": n_edges,
            "density": density,
            "is_directed": not is_symmetric,
            "sparsity": (1 - density),
        }

    def degree_analysis(self, sample_size=1000):
        """ä¼˜åŒ–çš„åº¦åˆ†å¸ƒåˆ†æï¼Œæ”¯æŒæŠ½æ ·"""
        print("\n" + "=" * 50)
        print("åº¦åˆ†å¸ƒåˆ†æ (ä¼˜åŒ–ç‰ˆ)")
        print("=" * 50)

        start_time = time.time()

        # ä½¿ç”¨ç¨€ç–çŸ©é˜µçš„å¿«é€Ÿåº¦è®¡ç®—
        if self._is_undirected():
            degrees = np.array(self.adj_matrix.sum(axis=1)).flatten()
        else:
            in_degrees = np.array(self.adj_matrix.sum(axis=0)).flatten()
            out_degrees = np.array(self.adj_matrix.sum(axis=1)).flatten()
            degrees = in_degrees + out_degrees

        # åŸºæœ¬ç»Ÿè®¡
        degree_stats = {
            "mean": np.mean(degrees),
            "std": np.std(degrees),
            "max": np.max(degrees),
            "min": np.min(degrees),
            "median": np.median(degrees),
        }

        print(f"å¹³å‡åº¦: {degree_stats['mean']:.2f}")
        print(f"åº¦æ ‡å‡†å·®: {degree_stats['std']:.2f}")
        print(f"æœ€å¤§åº¦: {degree_stats['max']}")
        print(f"æœ€å°åº¦: {degree_stats['min']}")
        print(f"åº¦ä¸­ä½æ•°: {degree_stats['median']}")

        # åº¦åˆ†å¸ƒæŠ½æ ·åˆ†æ
        if self.n_nodes > sample_size:
            sampled_indices = np.random.choice(self.n_nodes, sample_size, replace=False)
            sampled_degrees = degrees[sampled_indices]
            print(f"\nåŸºäº {sample_size} ä¸ªèŠ‚ç‚¹çš„æŠ½æ ·åˆ†æ:")
            print(f"  æŠ½æ ·å¹³å‡åº¦: {np.mean(sampled_degrees):.2f}")
            print(f"  æŠ½æ ·åº¦æ ‡å‡†å·®: {np.std(sampled_degrees):.2f}")

        elapsed_time = time.time() - start_time
        print(f"è®¡ç®—æ—¶é—´: {elapsed_time:.2f}ç§’")

        return degrees, degree_stats

    def _is_undirected(self):
        """æ£€æŸ¥å›¾æ˜¯å¦æ— å‘"""
        if sparse.issparse(self.adj_matrix):
            return (self.adj_matrix != self.adj_matrix.T).nnz == 0
        else:
            return np.allclose(self.adj_matrix, self.adj_matrix.T)

    def connected_components_analysis(self, max_components=10):
        """ä¼˜åŒ–çš„è¿é€šåˆ†é‡åˆ†æ"""
        print("\n" + "=" * 50)
        print("è¿é€šåˆ†é‡åˆ†æ (ä¼˜åŒ–ç‰ˆ)")
        print("=" * 50)

        if not self._is_undirected():
            print("æœ‰å‘å›¾çš„è¿é€šæ€§åˆ†æè¾ƒä¸ºå¤æ‚ï¼Œæ­¤å¤„çœç•¥")
            return None, None, None

        start_time = time.time()

        # ä½¿ç”¨scipyçš„è¿é€šåˆ†é‡ç®—æ³•ï¼ˆæ¯”è‡ªå®šä¹‰BFSå¿«å¾—å¤šï¼‰
        n_components, labels = sparse.csgraph.connected_components(
            self.adj_matrix, directed=False, return_labels=True
        )

        # è®¡ç®—å„è¿é€šåˆ†é‡å¤§å°
        component_sizes = Counter(labels)
        sorted_components = sorted(
            component_sizes.items(), key=lambda x: x[1], reverse=True
        )

        print(f"è¿é€šåˆ†é‡æ•°é‡: {n_components}")
        print(f"æœ€å¤§è¿é€šåˆ†é‡å¤§å°: {sorted_components[0][1]}")
        print(f"è¿é€šåˆ†é‡å¤§å°åˆ†å¸ƒ (å‰{min(max_components, n_components)}ä¸ª):")

        total_shown = 0
        for comp_id, size in sorted_components[:max_components]:
            print(f"  åˆ†é‡ {comp_id}: {size} ä¸ªèŠ‚ç‚¹")
            total_shown += size

        if n_components > max_components:
            remaining_nodes = self.n_nodes - total_shown
            print(
                f"  å…¶ä»– {n_components - max_components} ä¸ªåˆ†é‡: {remaining_nodes} ä¸ªèŠ‚ç‚¹"
            )

        # æ£€æŸ¥æ˜¯å¦è¿é€š
        is_connected = n_components == 1
        print(f"å›¾æ˜¯å¦è¿é€š: {is_connected}")

        elapsed_time = time.time() - start_time
        print(f"è®¡ç®—æ—¶é—´: {elapsed_time:.2f}ç§’")

        return n_components, labels, is_connected

    def centrality_analysis(self, top_k=10):
        """ä¼˜åŒ–çš„ä¸­å¿ƒæ€§åˆ†æ"""
        print("\n" + "=" * 50)
        print("ä¸­å¿ƒæ€§åˆ†æ (ä¼˜åŒ–ç‰ˆ)")
        print("=" * 50)

        start_time = time.time()

        # åªè®¡ç®—åº¦ä¸­å¿ƒæ€§ï¼ˆå…¶ä»–ä¸­å¿ƒæ€§è®¡ç®—æˆæœ¬å¤ªé«˜ï¼‰
        degrees = np.array(self.adj_matrix.sum(axis=1)).flatten()
        degree_centrality = degrees / (self.n_nodes - 1)

        print(f"åº¦ä¸­å¿ƒæ€§æœ€é«˜çš„ {top_k} ä¸ªèŠ‚ç‚¹:")
        top_indices = np.argpartition(degree_centrality, -top_k)[-top_k:]
        top_indices = top_indices[np.argsort(degree_centrality[top_indices])[::-1]]

        for i, node in enumerate(top_indices):
            print(f"  {i+1:2d}. èŠ‚ç‚¹ {node:5d}: {degree_centrality[node]:.6f}")

        elapsed_time = time.time() - start_time
        print(f"è®¡ç®—æ—¶é—´: {elapsed_time:.2f}ç§’")

        return degree_centrality

    def sampling_based_clustering(self, sample_size=1000):
        """åŸºäºæŠ½æ ·çš„èšç±»ç³»æ•°åˆ†æ"""
        print("\n" + "=" * 50)
        print("èšç±»ç³»æ•°åˆ†æ (æŠ½æ ·ç‰ˆ)")
        print("=" * 50)

        start_time = time.time()

        # æŠ½æ ·è®¡ç®—èšç±»ç³»æ•°
        if sample_size > self.n_nodes:
            sample_size = self.n_nodes

        sampled_indices = np.random.choice(self.n_nodes, sample_size, replace=False)
        clustering_coeffs = []

        for i in sampled_indices:
            neighbors = self.adj_matrix[i].nonzero()[1]
            k = len(neighbors)

            if k < 2:
                clustering_coeffs.append(0.0)
            else:
                # è®¡ç®—é‚»å±…ä¹‹é—´çš„è¾¹æ•°
                edges_between_neighbors = 0
                # åªæ£€æŸ¥éƒ¨åˆ†é‚»å±…å¯¹ä»¥é¿å…ç»„åˆçˆ†ç‚¸
                max_pairs = min(1000, k * (k - 1) // 2)
                if k > 50:  # å¯¹äºé«˜åº¦æ•°èŠ‚ç‚¹ï¼Œè¿›ä¸€æ­¥æŠ½æ ·
                    neighbor_pairs = []
                    for _ in range(max_pairs):
                        u, v = np.random.choice(neighbors, 2, replace=False)
                        if u != v and self.adj_matrix[u, v] > 0:
                            edges_between_neighbors += 1
                    coeff = (2 * edges_between_neighbors) / max_pairs
                else:
                    for u_idx, u in enumerate(neighbors):
                        for v in neighbors[u_idx + 1 :]:
                            if self.adj_matrix[u, v] > 0:
                                edges_between_neighbors += 1
                    coeff = (2 * edges_between_neighbors) / (k * (k - 1))

                clustering_coeffs.append(coeff)

        clustering_coeffs = np.array(clustering_coeffs)

        print(f"åŸºäº {sample_size} ä¸ªèŠ‚ç‚¹çš„æŠ½æ ·ç»“æœ:")
        print(f"å¹³å‡èšç±»ç³»æ•°: {np.mean(clustering_coeffs):.6f}")
        print(f"èšç±»ç³»æ•°æ ‡å‡†å·®: {np.std(clustering_coeffs):.6f}")
        print(f"èšç±»ç³»æ•°ä¸­ä½æ•°: {np.median(clustering_coeffs):.6f}")

        elapsed_time = time.time() - start_time
        print(f"è®¡ç®—æ—¶é—´: {elapsed_time:.2f}ç§’")

        return clustering_coeffs

    def efficient_visualization(self, max_nodes=1000):
        """é’ˆå¯¹å¤§å›¾çš„ç®€åŒ–å¯è§†åŒ–"""
        print("\n" + "=" * 50)
        print("ç®€åŒ–å¯è§†åŒ–")
        print("=" * 50)

        # å¦‚æœå›¾å¤ªå¤§ï¼Œåªå¯è§†åŒ–æœ€å¤§è¿é€šåˆ†é‡æˆ–æŠ½æ ·
        n_components, labels, _ = self.connected_components_analysis(max_components=5)

        if n_components == 1 and self.n_nodes > max_nodes:
            print("å›¾å¤ªå¤§ï¼Œè¿›è¡ŒæŠ½æ ·å¯è§†åŒ–...")
            # éšæœºæŠ½æ ·èŠ‚ç‚¹
            sample_nodes = np.random.choice(self.n_nodes, max_nodes, replace=False)
            subgraph = self.adj_matrix[sample_nodes, :][:, sample_nodes]
            G = nx.from_scipy_sparse_array(subgraph)
        else:
            # ä½¿ç”¨æœ€å¤§è¿é€šåˆ†é‡
            component_sizes = Counter(labels)
            largest_component_id = max(component_sizes, key=component_sizes.get)
            nodes_in_largest = np.where(labels == largest_component_id)[0]

            if len(nodes_in_largest) > max_nodes:
                print(f"æœ€å¤§è¿é€šåˆ†é‡æœ‰ {len(nodes_in_largest)} ä¸ªèŠ‚ç‚¹ï¼Œè¿›è¡ŒæŠ½æ ·...")
                nodes_in_largest = np.random.choice(
                    nodes_in_largest, max_nodes, replace=False
                )

            subgraph = self.adj_matrix[nodes_in_largest, :][:, nodes_in_largest]
            G = nx.from_scipy_sparse_array(subgraph)

        plt.figure(figsize=(15, 5))

        # åº¦åˆ†å¸ƒç›´æ–¹å›¾
        plt.subplot(1, 3, 1)
        degrees = [d for _, d in G.degree()]
        plt.hist(degrees, bins=30, alpha=0.7, color="skyblue", edgecolor="black")
        plt.xlabel("åº¦")
        plt.ylabel("é¢‘ç‡")
        plt.title("åº¦åˆ†å¸ƒ")

        # å›¾ç»“æ„å¯è§†åŒ–
        plt.subplot(1, 3, 2)
        pos = nx.spring_layout(G, seed=42)
        nx.draw(G, pos, node_size=20, alpha=0.6, edge_color="gray", width=0.5)
        plt.title("å›¾ç»“æ„")

        # é‚»æ¥çŸ©é˜µçƒ­å›¾ï¼ˆåªæ˜¾ç¤ºéƒ¨åˆ†ï¼‰
        plt.subplot(1, 3, 3)
        if subgraph.shape[0] > 500:
            # å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼Œè¿›ä¸€æ­¥æŠ½æ ·
            sample_idx = np.random.choice(subgraph.shape[0], 500, replace=False)
            subgraph = subgraph[sample_idx, :][:, sample_idx]

        sns.heatmap(subgraph.toarray(), cmap="Blues", cbar=True)
        plt.title("é‚»æ¥çŸ©é˜µæŠ½æ ·")

        plt.tight_layout()
        plt.show()

    def memory_usage_report(self):
        """å†…å­˜ä½¿ç”¨æŠ¥å‘Š"""
        print("\n" + "=" * 50)
        print("å†…å­˜ä½¿ç”¨æŠ¥å‘Š")
        print("=" * 50)

        if sparse.issparse(self.adj_matrix):
            dense_size = self.n_nodes * self.n_nodes * 8 / (1024**3)  # GB
            sparse_size = (
                self.adj_matrix.data.nbytes
                + self.adj_matrix.indices.nbytes
                + self.adj_matrix.indptr.nbytes
            ) / (
                1024**3
            )  # GB

            print(f"ç¨ å¯†çŸ©é˜µä¼°è®¡å¤§å°: {dense_size:.2f} GB")
            print(f"ç¨€ç–çŸ©é˜µå®é™…å¤§å°: {sparse_size:.2f} GB")
            print(f"å†…å­˜èŠ‚çœ: {(1 - sparse_size/dense_size)*100:.1f}%")

    def comprehensive_analysis(self, visualize=True, sample_size=1000):
        """æ‰§è¡Œå…¨é¢çš„ä¼˜åŒ–åˆ†æ"""
        print("å¼€å§‹ç¨€ç–å›¾æ¢ç´¢æ€§åˆ†æ (ä¼˜åŒ–ç‰ˆ)...")

        # å†…å­˜æŠ¥å‘Š
        self.memory_usage_report()

        # åŸºæœ¬ç»Ÿè®¡
        basic_stats = self.basic_statistics()

        # åº¦åˆ†æ
        degrees, degree_stats = self.degree_analysis(sample_size)

        # è¿é€šæ€§åˆ†æ
        connectivity_results = self.connected_components_analysis()

        # ä¸­å¿ƒæ€§åˆ†æ
        centrality = self.centrality_analysis()

        # èšç±»ç³»æ•°åˆ†æï¼ˆæŠ½æ ·ï¼‰
        clustering = self.sampling_based_clustering(sample_size)

        # ç®€åŒ–å¯è§†åŒ–
        if visualize and self.n_nodes <= 10000:  # åªåœ¨èŠ‚ç‚¹æ•°é€‚ä¸­æ—¶å¯è§†åŒ–
            self.efficient_visualization()
        elif visualize:
            print("\nå›¾å¤ªå¤§ï¼Œè·³è¿‡è¯¦ç»†å¯è§†åŒ–")
            if input("æ˜¯å¦æ˜¾ç¤ºç®€åŒ–æŠ½æ ·å¯è§†åŒ–? (y/n): ").lower() == "y":
                self.efficient_visualization()

        # è¿”å›æ‰€æœ‰åˆ†æç»“æœ
        return {
            "basic_stats": basic_stats,
            "degrees": degrees,
            "degree_stats": degree_stats,
            "connectivity": connectivity_results,
            "centrality": centrality,
            "clustering": clustering,
        }


def compute_communitude_metric(A, labels, axis=0):
    """
    Calculate the communitude metric for each community to compare intra-layer and inter-layer community quality.
    """
    A = np.array(A)
    labels = np.array(labels)
    total_edge_weight = np.sum(A)
    unique_communities = np.unique(labels)
    results = {}

    for ck in unique_communities:
        if axis == 0:
            rows_in_ck = np.where(labels == ck)[0]
            submatrix = A[np.ix_(rows_in_ck, list(range(A.shape[1])))]
            e_intra_ck = np.sum(submatrix)
            e_inter_ck = np.sum(A[rows_in_ck, :]) - e_intra_ck
        else:
            cols_in_ck = np.where(labels == ck)[0]
            submatrix = A[np.ix_(list(range(A.shape[0])), cols_in_ck)]
            e_intra_ck = np.sum(submatrix)
            e_inter_ck = np.sum(A[:, cols_in_ck]) - e_intra_ck

        if total_edge_weight == 0:
            results[ck] = 0.0
            continue

        numerator = (e_intra_ck / total_edge_weight) - (
            (e_intra_ck + e_inter_ck) / (2 * total_edge_weight)
        ) ** 2
        denominator = ((e_intra_ck + e_inter_ck) / (2 * total_edge_weight)) ** 2 * (
            1 - ((e_intra_ck + e_inter_ck) / (2 * total_edge_weight)) ** 2
        )

        results[ck] = 0.0 if denominator == 0 else numerator / denominator

    return results


def create_mapping(row):
    if row["type"] == "intra":
        return 100 + row["community_id"]
    else:
        return 200 + row["community_id"]


if __name__ == "__main__":
    from sklearn.decomposition import NMF

    np.random.seed(42)
    simulated = np.random.randint(0, 2, (10, 10))
    model = NMF(n_components=4, init="random", random_state=42)
    U = model.fit_transform(simulated)
    Vt = model.components_
    V = Vt.T
    print(U.shape)  # (10, 4)
    labels = np.argmax(U, axis=1)  # (10,)
    unique_labels = np.unique(labels)
    total_edge_weight = np.sum(simulated)
    # print(unique_labels)

    results = {}
    for ul in unique_labels:
        rows_in_ul = np.where(labels == ul)[0]
        # row_in_ul æ˜¯ ul ç±»åˆ«çš„æ‰€æœ‰è¡Œç´¢å¼•(å¯¹åº”èŠ‚ç‚¹id)
        # if ul == unique_labels[0]:
        #     print(np.where(labels == ul))
        # print(ul,rows_in_ul)
        submatrix = simulated[np.ix_(rows_in_ul, list(range(simulated.shape[1])))]
        # np.ix_ : ç”¨äºç”Ÿæˆä¸€ä¸ªäºŒç»´çš„ç´¢å¼•æ•°ç»„ï¼Œç”¨äºä»çŸ©é˜µä¸­æå–å­çŸ©é˜µ
        # è¿™é‡Œçš„ np.ix_(rows_in_ul, list(range(simulated.shape[1]))) è¡¨ç¤ºæå– simulated çŸ©é˜µä¸­ rows_in_ul è¡Œå’Œæ‰€æœ‰åˆ—çš„å­çŸ©é˜µ
        # å³æå–èŠ‚ç‚¹15è¿æ¥åˆ°çš„ç‚¹çš„ç´¢å¼•(åˆ—ç´¢å¼•)
        # å†ç”¨ np.where(submatrix == 1)[1] æ‰¾åˆ°èŠ‚ç‚¹15è¿æ¥åˆ°çš„ç‚¹çš„ç´¢å¼•(åˆ—ç´¢å¼•)
        # å³èŠ‚ç‚¹15è¿æ¥åˆ°çš„ç‚¹çš„ç´¢å¼•(åˆ—ç´¢å¼•) = [15, 22, 24, 31, 32, 34, 37, 40, 42, 44]
        # å†ç”¨ np.where(simulated[rows_in_ul, :] == 1)[1] æ‰¾åˆ°èŠ‚ç‚¹15è¿æ¥åˆ°çš„ç‚¹çš„ç´¢å¼•(åˆ—ç´¢å¼•)
        # å³èŠ‚ç‚¹15è¿æ¥åˆ°çš„ç‚¹çš„ç´¢å¼•(åˆ—ç´¢å¼•) = [15, 22, 24, 31, 32, 34, 37, 40, 42, 44]
        # å†ç”¨ np.where(U[rows_in_ul, :] == 1)[1] æ‰¾åˆ°èŠ‚ç‚¹15è¿æ¥åˆ°çš„ç‚¹çš„ç´¢å¼•(åˆ—ç´¢å¼•)
        # å³èŠ‚ç‚¹15è¿æ¥åˆ°çš„ç‚¹çš„ç´¢å¼•(åˆ—ç´¢å¼•) = [2, 3, 4, 9]
        # è¿™é‡Œçš„ np.ix_(rows_in_ul, list(range(U.shape[1]))) è¡¨ç¤ºæå– U çŸ©é˜µä¸­ rows_in_ul è¡Œå’Œæ‰€æœ‰åˆ—çš„å­çŸ©é˜µ
        # å³æå–èŠ‚ç‚¹15è¿æ¥åˆ°çš„ç‚¹çš„ç´¢å¼•(åˆ—ç´¢å¼•) = [2, 3, 4, 9]
        e_intra_ul = np.sum(submatrix)
        e_inter_ul = np.sum(simulated[rows_in_ul, :]) - e_intra_ul
        if ul == unique_labels[0]:
            print(rows_in_ul)  # = [2 3 4 9]
            print(list(range(simulated.shape[1])))  # = [0, 1, 2, ..., 9]
            print(np.ix_(rows_in_ul, list(range(simulated.shape[1]))))
            # (array([[2],
            #        [3],
            #        [4],
            #        [9]]), array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]))

            print(submatrix)  # ç´¢å¼• rows_in_ul å„è¡Œçš„å…ƒç´ ï¼Œå³èŠ‚ç‚¹è¿æ¥åˆ°çš„ç‚¹çš„ç´¢å¼•
            # [[1 0 1 1 1 1 1 1 1 1]
            #  [0 0 1 1 1 0 1 0 0 0]
            #  [0 0 1 1 1 1 1 0 1 1]
            #  [1 1 1 1 1 1 1 1 1 0]]
            print(np.where(submatrix == 1))  # æ‰¾åˆ°èŠ‚ç‚¹15è¿æ¥åˆ°çš„ç‚¹çš„ç´¢å¼•
            print(np.where(submatrix == 1)[1])  # æ‰¾åˆ°èŠ‚ç‚¹15è¿æ¥åˆ°çš„ç‚¹çš„ç´¢å¼•(åˆ—ç´¢å¼•)
