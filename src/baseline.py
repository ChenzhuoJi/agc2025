"""
Author: PumpkinğŸƒ
Date:2025-11-10
Description: å¯¹æ¯”å®éªŒ
"""

import json
import os
import time
from typing import Dict, Tuple, List, Union

import numpy as np
import pandas as pd
import networkx as nx

from scipy.linalg import eigh
from scipy.sparse import csr_matrix
from sklearn.cluster import KMeans
from sklearn.decomposition import NMF

from src.processor import feature_process, edge_process

from cdlib import algorithms


def community2nodelabels(
    communities: List[List[int]], all_node_ids: List[int]
) -> np.ndarray:
    """
    å°†ç¤¾åŒºåˆ—è¡¨è½¬æ¢ä¸º cluster_labels æ•°ç»„ã€‚

    Args:
        communities (List[List[int]]): ç¤¾åŒºåˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨åŒ…å«ä¸€ä¸ªç¤¾åŒºçš„èŠ‚ç‚¹IDã€‚
        all_node_ids (List[int]): å›¾ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„IDåˆ—è¡¨ã€‚è¿™ä¸ªåˆ—è¡¨çš„é¡ºåºå†³å®šäº†è¾“å‡ºæ•°ç»„çš„ç´¢å¼•é¡ºåºã€‚
                                   é€šå¸¸ï¼Œè¿™å¯ä»¥é€šè¿‡ sorted(graph.nodes()) è·å¾—ã€‚

    Returns:
        Union[np.ndarray, List[int]]: cluster_labels æ•°ç»„ã€‚æ•°ç»„çš„ç´¢å¼•å¯¹åº” `all_node_ids` ä¸­çš„èŠ‚ç‚¹é¡ºåºï¼Œ
                                      æ•°ç»„çš„å€¼æ˜¯è¯¥èŠ‚ç‚¹çš„ç¤¾åŒºIDã€‚å¦‚æœèŠ‚ç‚¹IDä¸åœ¨ç¤¾åŒºåˆ—è¡¨ä¸­ï¼Œå…¶å€¼ä¸º -1ã€‚
                                      é»˜è®¤è¿”å› NumPy æ•°ç»„ï¼Œè‹¥éœ€è¿”å›åˆ—è¡¨ï¼Œå¯è®¾ç½® return_as_list=Trueã€‚
    """
    # 1. é¦–å…ˆåˆ›å»ºä¸€ä¸ªèŠ‚ç‚¹åˆ°ç¤¾åŒºIDçš„æ˜ å°„å­—å…¸ï¼Œè¿™æ˜¯é«˜æ•ˆæŸ¥æ‰¾çš„å…³é”®
    node_to_community = {}
    for community_id, community_nodes in enumerate(communities):
        for node_id in community_nodes:
            node_to_community[node_id] = community_id

    # 2. åˆ›å»º cluster_labels æ•°ç»„
    # éå†æ‰€æœ‰èŠ‚ç‚¹IDï¼Œå¹¶æ ¹æ®æ˜ å°„å­—å…¸ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…ç¤¾åŒºID
    # å¦‚æœèŠ‚ç‚¹ä¸åœ¨ä»»ä½•ç¤¾åŒºä¸­ï¼Œåˆ™åˆ†é… -1
    cluster_labels = [node_to_community.get(node_id, -1) for node_id in all_node_ids]
    # 3. è½¬æ¢ä¸º NumPy æ•°ç»„å¹¶è¿”å›ï¼ˆæ¨èï¼Œä¾¿äºåç»­å¤„ç†ï¼‰
    return np.array(cluster_labels)


def load_graph_and_features(
    edges_filepath: str,
    features_filepath: str,
    feature_file_type: str = "csv",  # æ–°å¢å‚æ•°ï¼šæŒ‡å®šç‰¹å¾æ–‡ä»¶ç±»å‹ï¼Œ"csv"æˆ–"json"
) -> Tuple[nx.Graph, Dict]:
    """
    ä»è¾¹æ–‡ä»¶å’Œç‰¹å¾æ–‡ä»¶ï¼ˆCSV/JSONï¼‰åŠ è½½æ•°æ®ï¼Œè½¬æ¢ä¸º networkx.Graph å’ŒèŠ‚ç‚¹å±æ€§å­—å…¸ã€‚

    Args:
        edges_filepath (str): .edges æ–‡ä»¶è·¯å¾„ï¼ˆCSVæ ¼å¼ï¼Œæ¯è¡Œä¸¤ä¸ªèŠ‚ç‚¹IDï¼‰ã€‚
        features_filepath (str): .features æ–‡ä»¶è·¯å¾„ï¼ˆCSVæˆ–JSONæ ¼å¼ï¼‰ã€‚
        feature_file_type (str): ç‰¹å¾æ–‡ä»¶ç±»å‹ï¼Œå¯é€‰ "csv" æˆ– "json"ï¼Œé»˜è®¤ "csv"ã€‚

    Returns:
        tuple: (G, node_attributes)
            - G (networkx.Graph): æ— å‘å›¾å¯¹è±¡ã€‚
            - node_attributes (dict): èŠ‚ç‚¹å±æ€§å­—å…¸ï¼Œæ ¼å¼ {node_id: {'feat_xxx': 0/1, ...}}ã€‚

    Raises:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨ã€‚
        ValueError: ç‰¹å¾æ–‡ä»¶ç±»å‹æ— æ•ˆã€‚
        json.JSONDecodeError: JSONæ–‡ä»¶æ ¼å¼é”™è¯¯ã€‚
    """
    # --- 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ ---
    for filepath in [edges_filepath, features_filepath]:
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"æ–‡ä»¶æœªæ‰¾åˆ°: {filepath}")

    # --- 2. åŠ è½½å¹¶æ„å»ºå›¾ï¼ˆä¸ä¹‹å‰é€»è¾‘ä¸€è‡´ï¼‰ ---
    print(f"æ­£åœ¨åŠ è½½è¾¹æ–‡ä»¶: {edges_filepath}")
    edges_df = pd.read_csv(edges_filepath, header=None, names=["source", "target"])
    G = nx.from_pandas_edgelist(edges_df, "source", "target")
    print(f"å›¾æ„å»ºå®Œæˆï¼š{G.number_of_nodes()} ä¸ªèŠ‚ç‚¹ï¼Œ{G.number_of_edges()} æ¡è¾¹")

    # --- 3. åŠ è½½å¹¶æ„å»ºèŠ‚ç‚¹å±æ€§å­—å…¸ï¼ˆæ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†ï¼‰ ---
    print(f"æ­£åœ¨åŠ è½½ç‰¹å¾æ–‡ä»¶: {features_filepath}ï¼ˆç±»å‹ï¼š{feature_file_type}ï¼‰")
    if feature_file_type == "csv":
        # åŸæœ‰CSVæ ¼å¼å¤„ç†é€»è¾‘
        features_df = pd.read_csv(features_filepath, header=None)
        node_attributes = {}
        for idx, row in features_df.iterrows():
            node_id = idx  # å‡è®¾èŠ‚ç‚¹ID=è¡Œå·ï¼ˆ0-basedï¼‰ï¼Œéœ€è°ƒæ•´åˆ™æ”¹ä¸º idx+1ï¼ˆ1-basedï¼‰
            node_attributes[node_id] = {f"attr_{i}": val for i, val in enumerate(row)}
    elif feature_file_type == "json":
        # æ–°å¢JSONæ ¼å¼å¤„ç†é€»è¾‘
        with open(features_filepath, "r", encoding="utf-8") as f:
            feat_dict = json.load(f)  # è¯»å–ä¸º {èŠ‚ç‚¹id: [æ‹¥æœ‰çš„ç‰¹å¾idåˆ—è¡¨]}

        # ç¬¬ä¸€æ­¥ï¼šè·å–æ‰€æœ‰ç‰¹å¾IDï¼ˆç”¨äºè¡¥å…¨"æ— ç‰¹å¾"çš„å±æ€§ä¸º0ï¼‰
        all_feature_ids = set()
        for feat_list in feat_dict.values():
            all_feature_ids.update(feat_list)
        all_feature_ids = sorted(list(all_feature_ids))  # æ’åºä¿è¯å±æ€§é¡ºåºä¸€è‡´
        print(f"å…±æ£€æµ‹åˆ° {len(all_feature_ids)} ä¸ªä¸åŒç‰¹å¾")

        # ç¬¬äºŒæ­¥ï¼šæ„å»ºå±æ€§å­—å…¸ï¼ˆæœ‰ç‰¹å¾=1ï¼Œæ— ç‰¹å¾=0ï¼‰
        node_attributes = {}
        for node_id_str, owned_feats in feat_dict.items():
            # èŠ‚ç‚¹IDå¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼ˆJSONé”®é»˜è®¤å­—ç¬¦ä¸²ï¼‰ï¼Œè½¬æ¢ä¸ºæ•´æ•°ï¼ˆä¸è¾¹æ–‡ä»¶èŠ‚ç‚¹IDç±»å‹ä¸€è‡´ï¼‰
            node_id = int(node_id_str)
            # ä¸ºå½“å‰èŠ‚ç‚¹åˆå§‹åŒ–æ‰€æœ‰ç‰¹å¾ä¸º0
            attrs = {f"feat_{fid}": 0 for fid in all_feature_ids}
            # å¯¹æ‹¥æœ‰çš„ç‰¹å¾ï¼Œè®¾ä¸º1
            for fid in owned_feats:
                attrs[f"feat_{fid}"] = 1
            node_attributes[node_id] = attrs
    else:
        raise ValueError(
            f"æ— æ•ˆçš„ç‰¹å¾æ–‡ä»¶ç±»å‹ï¼š{feature_file_type}ï¼Œä»…æ”¯æŒ 'csv' æˆ– 'json'"
        )

    print(f"ç‰¹å¾åŠ è½½å®Œæˆï¼šä¸º {len(node_attributes)} ä¸ªèŠ‚ç‚¹åˆ†é…å±æ€§")

    # --- 4. éªŒè¯å¹¶æ¸…ç†æ•°æ®ï¼ˆç¡®ä¿å›¾èŠ‚ç‚¹éƒ½æœ‰å±æ€§ï¼‰ ---
    graph_nodes = set(G.nodes())
    attr_nodes = set(node_attributes.keys())
    nodes_without_attrs = graph_nodes - attr_nodes
    nodes_without_graph = attr_nodes - graph_nodes
    if nodes_without_attrs:
        print(f"è­¦å‘Šï¼š{len(nodes_without_attrs)} ä¸ªå›¾èŠ‚ç‚¹æ— å¯¹åº”ç‰¹å¾ï¼Œå°†ç§»é™¤")
        G.remove_nodes_from(nodes_without_attrs)
    if nodes_without_graph:
        print(f"è­¦å‘Šï¼š{len(nodes_without_graph)} ä¸ªç‰¹å¾èŠ‚ç‚¹ä¸åœ¨å›¾ä¸­ï¼Œå°†å¿½ç•¥")

    print(f"æœ€ç»ˆå›¾ï¼š{G.number_of_nodes()} ä¸ªèŠ‚ç‚¹ï¼Œ{G.number_of_edges()} æ¡è¾¹")
    return G, node_attributes


def spectral_clustering(X, k):
    if isinstance(X, csr_matrix):
        X = X.toarray()
    similarity_matrix = X
    # è®¡ç®—åº¦çŸ©é˜µ D
    degree_matrix = np.sum(similarity_matrix, axis=1)
    D = np.diag(degree_matrix)

    # è®¡ç®—è§„èŒƒåŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ L_sym
    D_inv_sqrt = np.diag(1.0 / np.sqrt(degree_matrix))
    L = D - similarity_matrix
    L_norm = np.dot(np.dot(D_inv_sqrt, L), D_inv_sqrt)
    L_norm = np.nan_to_num(L_norm, nan=0.0, posinf=1e10, neginf=-1e10)
    # ç‰¹å¾å€¼åˆ†è§£
    eigvals, eigvecs = eigh(L_norm)

    # é€‰æ‹©æœ€å°çš„ k ä¸ªç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡
    embedding = eigvecs[:, :k]

    # ä½¿ç”¨ K-means èšç±»
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(embedding)

    return labels


def nmf_clustering(X, k):
    if isinstance(X, csr_matrix):
        X = X.toarray()
    # åˆå§‹åŒ– NMF æ¨¡å‹
    nmf = NMF(n_components=k, init="random", random_state=42)

    # å¯¹æ•°æ®è¿›è¡Œéè´ŸçŸ©é˜µåˆ†è§£
    W = nmf.fit_transform(X)

    # ä½¿ç”¨ K-means èšç±»
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(W)

    return labels


def baselineExperiment(dataname):
    result = {}
    if "llf" in dataname:
        features_format = "csv"
    else:
        features_format = "json"

    features_file = f"data/graphs/{dataname}.features"

    G, node_attrs = load_graph_and_features(
        edges_filepath=f"data/graphs/{dataname}.edges",
        features_filepath=features_file,
        feature_file_type=features_format,  # å…³é”®ï¼šæŒ‡å®šä¸ºJSONç±»å‹
    )
    targets = pd.read_csv(f"data/graphs/{dataname}.targets", header=None)
    time_start = time.time()
    comminities = algorithms.ilouvain(G, node_attrs)

    time_end = time.time()
    print(f"ilouvain è¿è¡Œæ—¶é—´: {time_end - time_start} ç§’")
    result["ilouvain"] = {
        "time": time_end - time_start,
    }
    cluster_labels = community2nodelabels(
        comminities.communities, sorted(list(G.nodes()))
    )
    if os.path.exists(f"results/baseline/{dataname}.baseline"):
        baseline_metrics = pd.read_csv(f"results/baseline/{dataname}.baseline", header=None)
        baseline_metrics['ilouvain'] = cluster_labels
        with open(f"results/baseline/{dataname}.time", "a", encoding="utf-8") as f:
            f.write(f"ilouvain_time: {time_end - time_start} seconds\n")
            
    else:
        print("baseline æŒ‡æ ‡æ–‡ä»¶ä¸å­˜åœ¨")



    # time_start = time.time()
    # comminities = algorithms.eva(G, node_attrs)
    # time_end = time.time()
    # print(f"eva è¿è¡Œæ—¶é—´: {time_end - time_start} ç§’")
    # result["eva"] = {
    #     "time": time_end - time_start,
    # }
    # cluster_labels = community2nodelabels(
    #     comminities.communities, sorted(list(G.nodes()))
    # )
    # eva = Evaluator(cluster_labels, targets.values[:, 1])
    # result["eva"].update(eva.get_all_metrics())
    
    return 

if __name__ == "__main__":
    print(baselineExperiment("llf_friendship"))
